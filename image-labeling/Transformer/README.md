The Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer architecture allows for significantly more parallelization and can reach new state of the art results in translation quality.

This Transformer uses the architecture defined in the [Attention is all you need](https://arxiv.org/abs/1706.03762) paper.
I have implemented a BLEU evaluation metric along with a Greedy Search approach.

Link to the Article :- [Implementation of Attention Mechanism for Caption Generation on Transformers using TensorFlow](https://www.analyticsvidhya.com/blog/2021/01/implementation-of-attention-mechanism-for-caption-generation-on-transformers-using-tensorflow/)


