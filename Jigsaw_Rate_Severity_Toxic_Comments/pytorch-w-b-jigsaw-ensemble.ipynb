{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n<h1 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f5 ; color : #fe346e; text-align: center; border-radius: 100px 100px;\">Jigsaw Pytorch Starter</h1>\n<br>\n\n<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAT4AAACfCAMAAABX0UX9AAAAe1BMVEUAAAD///93d3dxcXEfHx98fHyMjIyCgoKTk5Pt7e2goKD09PSFhYUiIiJ/f3/p6em0tLSmpqbJycni4uJVVVViYmKwsLA8PDzc3NwmJiYtLS3W1tYzMzMbGxtISEj39/ehoaHExMSYmJhBQUEMDAxmZmbFxcVaWloLCwtmYPo6AAAEDElEQVR4nO3ZC3eiOhDAcdD6wgcIFRWsivWx3/8TLjNJAF27671ntZd7/r9zug0hYhkzmch6HgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAB8j8iv9b77j2mdaSN6fue7/5rWiX4Xvo83NT163lvNnlx2gmC2bQ7fzoL94L3ZM5VXa2NqfrurVgdlc+q11qcfjr8M34fr73teWA1a6blTYo5GEzd6MDI9u3l1ga52nDzzKQ1t7046bczey2b0zBt8rqP89F0Eb8JXzcwyfKMqfMnVKd+PbwbbESLWw3PZ2tbdS+0cmIO0bF6efZNPF90L37qKxy+zT8cnUZT7bvZ0NJLzfdGIqGdfIU0J/0Y7J9pXmBHlBzd6zS0+VXAnfD/034EJX2nou9p8qgZL8vmyAEogF9KzlkAtddhMImlHyhsctLcwSa/tRdmqc73F8jvJq67DN9PWqmzZNU/mUuo1p1G3WubKa46z8ij3ZJF12WtXikzaMos/n3ZPL7T9B+HL/EbKzedzOT2u5pyXZZlW5zdNbMl6mceJHbBsTEl5VeL9L4SPh29yJ+V09sbD5kZmqGXhYF8vR3vPZLSrt5cqo1uveDx8QRVHQxbERVV2U7fzS3SOrm3Wyv48NO8zlkIi01dy9+Ppd/YS6ePhS10NsREbS3tWlWY/103d0q6KkrWSzLnJ3pFU3cIsfr7bRbZf9O/DZ9bBH8N81+wIbFnol78Dz6V8plVHLtrVAt5/wa29wurx8B1cbY2iKA2bO7fNINL9tQyTxbSsK/PIZq2nce3pHDQzs7BVpZU2V0frm8qbh2Gujcmv4TvZzYgyK9yx/k47N0m79ZukpEi9zcpg7jwtuaHEM/Za6vN6t5/ehE8mY3VC98ONfZ9f71K2Jlff6zxcmrKaXoVPCrVsCPeJCZkWqi/meyucdUFy9HtE82bmLh6ynh21q1+NKFw+mhQdmacLtkeifND5NQ7myq2G5aXG9rJyrbH7hFpo4ze3XHbb0QifJnNw6SY2w6I4luYqjlOzb/Z3h8XisHOVQjIznLxvT/p1eKPTcW+vJTVXngsU5l3c5tk3TxNa6Sx/fW52uZl7WtJMpaDOvLV3+8SlW5/MTfiOjedeMq+jOr91qsku2XxIO+0zRXrxuhv+q+yn74/OaZFUt321Ern4jfRLaR0d/ZZ1seEM17Hd933k1QjJzjqZ7XNDaWjMTNExz7Jec7N/39m/53ohz4I4WUW2rzvrGbOu6ehEeR6V7cWsZ4dsg/MqzKOBrJTrQW9WP3fu9noT2TlfyovYB9RbabZ98v02fPhKfj98/E/bQ46d7j2dzZ9fCgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOA/4CcgHyoagKr9FwAAAABJRU5ErkJggg==\" width=\"500\" height=\"600\">","metadata":{}},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Install Required Libraries</h1></span>","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade wandb","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-09T05:47:06.087157Z","iopub.execute_input":"2021-11-09T05:47:06.087591Z","iopub.status.idle":"2021-11-09T05:47:18.070011Z","shell.execute_reply.started":"2021-11-09T05:47:06.087481Z","shell.execute_reply":"2021-11-09T05:47:18.069199Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.12.5)\nCollecting wandb\n  Downloading wandb-0.12.6-py2.py3-none-any.whl (1.7 MB)\n\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.7 MB 924 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.23)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (8.0.1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (5.4.1)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.16.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.4.3)\nRequirement already satisfied: configparser>=3.8.1 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.0.2)\nRequirement already satisfied: yaspin>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.1.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.8.0)\nRequirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.19.0)\nRequirement already satisfied: subprocess32>=3.5.3 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.5.4)\nRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.0.1)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.3)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.8.0)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.25.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.8.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.7)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\nRequirement already satisfied: smmap<5,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\nRequirement already satisfied: termcolor<2.0.0,>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from yaspin>=1.0.0->wandb) (1.1.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.5.0)\nInstalling collected packages: wandb\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.12.5\n    Uninstalling wandb-0.12.5:\n      Successfully uninstalled wandb-0.12.5\nSuccessfully installed wandb-0.12.6\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Import Required Libraries üìö</h1></span>","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport copy\nimport time\nimport random\nimport string\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\n# Utils\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Sklearn Imports\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel, AdamW\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\ny_ = Fore.YELLOW\nsr_ = Style.RESET_ALL\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2021-11-09T05:47:18.073887Z","iopub.execute_input":"2021-11-09T05:47:18.074460Z","iopub.status.idle":"2021-11-09T05:47:24.395932Z","shell.execute_reply.started":"2021-11-09T05:47:18.074427Z","shell.execute_reply":"2021-11-09T05:47:24.395221Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\"> Weights & Biases (W&B) is a set of machine learning tools that helps you build better models faster. <strong>Kaggle competitions require fast-paced model development and evaluation</strong>. There are a lot of components: exploring the training data, training different models, combining trained models in different combinations (ensembling), and so on.</span>\n\n> <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">‚è≥ Lots of components = Lots of places to go wrong = Lots of time spent debugging</span>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">W&B can be useful for Kaggle competition with it's lightweight and interoperable tools:</span>\n\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Quickly track experiments,<br></span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Version and iterate on datasets, <br></span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Evaluate model performance,<br></span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Reproduce models,<br></span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Visualize results and spot regressions,<br></span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Share findings with colleagues.</span>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">To learn more about Weights and Biases check out this <strong><a href=\"https://www.kaggle.com/ayuraj/experiment-tracking-with-weights-and-biases\">kernel</a></strong>.</span>","metadata":{}},{"cell_type":"code","source":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    anony = None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T05:47:24.397233Z","iopub.execute_input":"2021-11-09T05:47:24.397512Z","iopub.status.idle":"2021-11-09T05:47:25.875585Z","shell.execute_reply.started":"2021-11-09T05:47:24.397479Z","shell.execute_reply":"2021-11-09T05:47:25.874834Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Training Configuration ‚öôÔ∏è</h1></span>","metadata":{}},{"cell_type":"code","source":"def id_generator(size=12, chars=string.ascii_lowercase + string.digits):\n    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))\n\nHASH_NAME = id_generator(size=12)\nprint(HASH_NAME)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T05:47:25.878169Z","iopub.execute_input":"2021-11-09T05:47:25.878445Z","iopub.status.idle":"2021-11-09T05:47:25.884680Z","shell.execute_reply.started":"2021-11-09T05:47:25.878408Z","shell.execute_reply":"2021-11-09T05:47:25.883806Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"k5nu8k69390a\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Each experiments are grouped together using the hash-value<br></span>\n\n![](https://i.imgur.com/Maej42h.jpg)","metadata":{}},{"cell_type":"code","source":"CONFIG = {\"seed\": 2021,\n          \"epochs\": 3,\n          \"model_name\": \"roberta-base\",\n          \"train_batch_size\": 32,\n          \"valid_batch_size\": 64,\n          \"max_length\": 128,\n          \"learning_rate\": 1e-4,\n          \"scheduler\": 'CosineAnnealingLR',\n          \"min_lr\": 1e-6,\n          \"T_max\": 500,\n          \"weight_decay\": 1e-6,\n          \"n_fold\": 5,\n          \"n_accumulate\": 1,\n          \"num_classes\": 1,\n          \"margin\": 0.5,\n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n          \"hash_name\": HASH_NAME\n          }\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\nCONFIG['group'] = f'{HASH_NAME}-Baseline'","metadata":{"execution":{"iopub.status.busy":"2021-11-09T05:47:25.887269Z","iopub.execute_input":"2021-11-09T05:47:25.887751Z","iopub.status.idle":"2021-11-09T05:47:30.926758Z","shell.execute_reply.started":"2021-11-09T05:47:25.887715Z","shell.execute_reply":"2021-11-09T05:47:30.926023Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1192dabb51054897a2c4d9a0d3b33087"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e0c4891f00c4b019d235b276b5cd5e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80d5f99ba6c544ff9c78418899ed55a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d69fcc8253649798594d1286f6ba7c3"}},"metadata":{}}]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Set Seed for Reproducibility</h1></span>","metadata":{}},{"cell_type":"code","source":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2021-11-09T05:47:30.928003Z","iopub.execute_input":"2021-11-09T05:47:30.928245Z","iopub.status.idle":"2021-11-09T05:47:30.937131Z","shell.execute_reply.started":"2021-11-09T05:47:30.928212Z","shell.execute_reply":"2021-11-09T05:47:30.936363Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Read the Data üìñ</h1>","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-09T05:47:30.938312Z","iopub.execute_input":"2021-11-09T05:47:30.939248Z","iopub.status.idle":"2021-11-09T05:47:31.517938Z","shell.execute_reply.started":"2021-11-09T05:47:30.939207Z","shell.execute_reply":"2021-11-09T05:47:31.517177Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   worker                                         less_toxic  \\\n0     313            This article sucks \\n\\nwoo woo wooooooo   \n1     188  \"And yes, people should recognize that but the...   \n2      82   Western Media?\\n\\nYup, because every crime in...   \n3     347  And you removed it! You numbskull! I don't car...   \n4     539   smelly vagina \\n\\nBluerasberry why don't you ...   \n\n                                          more_toxic  \n0  WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...  \n1   Daphne Guinness \\n\\nTop of the mornin' my fav...  \n2  \"Atom you don't believe actual photos of mastu...  \n3  You seem to have sand in your vagina.\\n\\nMight...  \n4           hey \\n\\nway to support nazis, you racist  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>worker</th>\n      <th>less_toxic</th>\n      <th>more_toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>313</td>\n      <td>This article sucks \\n\\nwoo woo wooooooo</td>\n      <td>WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>188</td>\n      <td>\"And yes, people should recognize that but the...</td>\n      <td>Daphne Guinness \\n\\nTop of the mornin' my fav...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>82</td>\n      <td>Western Media?\\n\\nYup, because every crime in...</td>\n      <td>\"Atom you don't believe actual photos of mastu...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>347</td>\n      <td>And you removed it! You numbskull! I don't car...</td>\n      <td>You seem to have sand in your vagina.\\n\\nMight...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>539</td>\n      <td>smelly vagina \\n\\nBluerasberry why don't you ...</td>\n      <td>hey \\n\\nway to support nazis, you racist</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Create Folds</h1></span>","metadata":{}},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG['seed'])\n\nfor fold, ( _, val_) in enumerate(skf.split(X=df, y=df.worker)):\n    df.loc[val_ , \"kfold\"] = int(fold)\n    \ndf[\"kfold\"] = df[\"kfold\"].astype(int)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-09T05:47:31.519060Z","iopub.execute_input":"2021-11-09T05:47:31.519549Z","iopub.status.idle":"2021-11-09T05:47:31.567281Z","shell.execute_reply.started":"2021-11-09T05:47:31.519510Z","shell.execute_reply":"2021-11-09T05:47:31.566545Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   worker                                         less_toxic  \\\n0     313            This article sucks \\n\\nwoo woo wooooooo   \n1     188  \"And yes, people should recognize that but the...   \n2      82   Western Media?\\n\\nYup, because every crime in...   \n3     347  And you removed it! You numbskull! I don't car...   \n4     539   smelly vagina \\n\\nBluerasberry why don't you ...   \n\n                                          more_toxic  kfold  \n0  WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...      4  \n1   Daphne Guinness \\n\\nTop of the mornin' my fav...      0  \n2  \"Atom you don't believe actual photos of mastu...      0  \n3  You seem to have sand in your vagina.\\n\\nMight...      2  \n4           hey \\n\\nway to support nazis, you racist      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>worker</th>\n      <th>less_toxic</th>\n      <th>more_toxic</th>\n      <th>kfold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>313</td>\n      <td>This article sucks \\n\\nwoo woo wooooooo</td>\n      <td>WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>188</td>\n      <td>\"And yes, people should recognize that but the...</td>\n      <td>Daphne Guinness \\n\\nTop of the mornin' my fav...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>82</td>\n      <td>Western Media?\\n\\nYup, because every crime in...</td>\n      <td>\"Atom you don't believe actual photos of mastu...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>347</td>\n      <td>And you removed it! You numbskull! I don't car...</td>\n      <td>You seem to have sand in your vagina.\\n\\nMight...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>539</td>\n      <td>smelly vagina \\n\\nBluerasberry why don't you ...</td>\n      <td>hey \\n\\nway to support nazis, you racist</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Dataset Class</h1></span>","metadata":{}},{"cell_type":"code","source":"class JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.more_toxic = df['more_toxic'].values\n        self.less_toxic = df['less_toxic'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        more_toxic = self.more_toxic[index]\n        less_toxic = self.less_toxic[index]\n        inputs_more_toxic = self.tokenizer.encode_plus(\n                                more_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        inputs_less_toxic = self.tokenizer.encode_plus(\n                                less_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        target = 1\n        \n        more_toxic_ids = inputs_more_toxic['input_ids']\n        more_toxic_mask = inputs_more_toxic['attention_mask']\n        \n        less_toxic_ids = inputs_less_toxic['input_ids']\n        less_toxic_mask = inputs_less_toxic['attention_mask']\n        \n        \n        return {\n            'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n            'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n            'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n            'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n            'target': torch.tensor(target, dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2021-11-09T05:47:31.568453Z","iopub.execute_input":"2021-11-09T05:47:31.568756Z","iopub.status.idle":"2021-11-09T05:47:31.579362Z","shell.execute_reply.started":"2021-11-09T05:47:31.568720Z","shell.execute_reply":"2021-11-09T05:47:31.578390Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Create Model</h1></span>","metadata":{}},{"cell_type":"code","source":"class JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, CONFIG['num_classes'])\n        \n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.drop(out[1])\n        outputs = self.fc(out)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2021-11-09T05:47:31.581016Z","iopub.execute_input":"2021-11-09T05:47:31.581288Z","iopub.status.idle":"2021-11-09T05:47:31.592156Z","shell.execute_reply.started":"2021-11-09T05:47:31.581254Z","shell.execute_reply":"2021-11-09T05:47:31.591436Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Loss Function</h1></span>\n\n![](https://i.imgur.com/qYwVt8V.jpg)\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Check the official documentation <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html\">here</a></span>","metadata":{}},{"cell_type":"code","source":"def criterion(outputs1, outputs2, targets):\n    return nn.MarginRankingLoss(margin=CONFIG['margin'])(outputs1, outputs2, targets)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T05:47:31.593376Z","iopub.execute_input":"2021-11-09T05:47:31.594186Z","iopub.status.idle":"2021-11-09T05:47:31.605110Z","shell.execute_reply.started":"2021-11-09T05:47:31.594149Z","shell.execute_reply":"2021-11-09T05:47:31.604507Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Training Function</h1></span>","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n        more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n        less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n        less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype=torch.long)\n        \n        batch_size = more_toxic_ids.size(0)\n\n        more_toxic_outputs = model(more_toxic_ids, more_toxic_mask)\n        less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n        \n        loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n        loss = loss / CONFIG['n_accumulate']\n        loss.backward()\n    \n        if (step + 1) % CONFIG['n_accumulate'] == 0:\n            optimizer.step()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    \n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2021-11-09T05:47:31.607298Z","iopub.execute_input":"2021-11-09T05:47:31.607522Z","iopub.status.idle":"2021-11-09T05:47:31.618363Z","shell.execute_reply.started":"2021-11-09T05:47:31.607499Z","shell.execute_reply":"2021-11-09T05:47:31.617555Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Validation Function</h1></span>","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:        \n        more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n        more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n        less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n        less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype=torch.long)\n        \n        batch_size = more_toxic_ids.size(0)\n\n        more_toxic_outputs = model(more_toxic_ids, more_toxic_mask)\n        less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n        \n        loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])   \n    \n    gc.collect()\n    \n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2021-11-09T05:47:31.619490Z","iopub.execute_input":"2021-11-09T05:47:31.620070Z","iopub.status.idle":"2021-11-09T05:47:31.631561Z","shell.execute_reply.started":"2021-11-09T05:47:31.620035Z","shell.execute_reply":"2021-11-09T05:47:31.630872Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Run Training</h1></span>","metadata":{}},{"cell_type":"code","source":"def run_training(model, optimizer, scheduler, device, num_epochs, fold):\n    # To automatically log gradients\n    wandb.watch(model, log_freq=100)\n    \n    if torch.cuda.is_available():\n        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch_loss = np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n                                           dataloader=train_loader, \n                                           device=CONFIG['device'], epoch=epoch)\n        \n        val_epoch_loss = valid_one_epoch(model, valid_loader, device=CONFIG['device'], \n                                         epoch=epoch)\n    \n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(val_epoch_loss)\n        \n        # Log the metrics\n        wandb.log({\"Train Loss\": train_epoch_loss})\n        wandb.log({\"Valid Loss\": val_epoch_loss})\n        \n        # deep copy the model\n        if val_epoch_loss <= best_epoch_loss:\n            print(f\"{b_}Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n            best_epoch_loss = val_epoch_loss\n            run.summary[\"Best Loss\"] = best_epoch_loss\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = f\"Loss-Fold-{fold}.bin\"\n            torch.save(model.state_dict(), PATH)\n            # Save a model file from the current directory\n            print(f\"Model Saved{sr_}\")\n            \n        print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2021-11-09T05:47:31.634766Z","iopub.execute_input":"2021-11-09T05:47:31.635177Z","iopub.status.idle":"2021-11-09T05:47:31.648350Z","shell.execute_reply.started":"2021-11-09T05:47:31.635139Z","shell.execute_reply":"2021-11-09T05:47:31.647519Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def prepare_loaders(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = JigsawDataset(df_train, tokenizer=CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n    valid_dataset = JigsawDataset(df_valid, tokenizer=CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n                              num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n                              num_workers=2, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","metadata":{"execution":{"iopub.status.busy":"2021-11-09T05:47:31.649524Z","iopub.execute_input":"2021-11-09T05:47:31.649875Z","iopub.status.idle":"2021-11-09T05:47:31.659691Z","shell.execute_reply.started":"2021-11-09T05:47:31.649835Z","shell.execute_reply":"2021-11-09T05:47:31.658992Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def fetch_scheduler(optimizer):\n    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n                                                   eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n                                                             eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == None:\n        return None\n        \n    return scheduler","metadata":{"execution":{"iopub.status.busy":"2021-11-09T05:47:31.661095Z","iopub.execute_input":"2021-11-09T05:47:31.661360Z","iopub.status.idle":"2021-11-09T05:47:31.671599Z","shell.execute_reply.started":"2021-11-09T05:47:31.661313Z","shell.execute_reply":"2021-11-09T05:47:31.670857Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Start Training</span>","metadata":{}},{"cell_type":"code","source":"for fold in range(0, CONFIG['n_fold']):\n    print(f\"{y_}====== Fold: {fold} ======{sr_}\")\n    run = wandb.init(project='Jigsaw', \n                     config=CONFIG,\n                     job_type='Train',\n                     group=CONFIG['group'],\n                     tags=['roberta-base', f'{HASH_NAME}', 'margin-loss'],\n                     name=f'{HASH_NAME}-fold-{fold}',\n                     anonymous='must')\n    \n    # Create Dataloaders\n    train_loader, valid_loader = prepare_loaders(fold=fold)\n    \n    model = JigsawModel(CONFIG['model_name'])\n    model.to(CONFIG['device'])\n    \n    # Define Optimizer and Scheduler\n    optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n    scheduler = fetch_scheduler(optimizer)\n    \n    model, history = run_training(model, optimizer, scheduler,\n                                  device=CONFIG['device'],\n                                  num_epochs=CONFIG['epochs'],\n                                  fold=fold)\n    \n    run.finish()\n    \n    del model, history, train_loader, valid_loader\n    _ = gc.collect()\n    print()","metadata":{"execution":{"iopub.status.busy":"2021-11-09T05:47:31.673501Z","iopub.execute_input":"2021-11-09T05:47:31.674351Z","iopub.status.idle":"2021-11-09T08:42:08.433031Z","shell.execute_reply.started":"2021-11-09T05:47:31.674262Z","shell.execute_reply":"2021-11-09T08:42:08.432258Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdchanda\u001b[0m (use `wandb login --relogin` to force relogin)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[33m====== Fold: 0 ======\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                    Syncing run <strong><a href=\"https://wandb.ai/dchanda/Jigsaw/runs/2qtiyn5b\" target=\"_blank\">k5nu8k69390a-fold-0</a></strong> to <a href=\"https://wandb.ai/dchanda/Jigsaw\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b46975a3055a4b34b6a92fe24c83c4e9"}},"metadata":{}},{"name":"stdout","text":"[INFO] Using GPU: Tesla P100-PCIE-16GB\n\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 752/752 [10:42<00:00,  1.17it/s, Epoch=1, LR=5.11e-5, Train_Loss=0.375]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:48<00:00,  1.97it/s, Epoch=1, LR=5.11e-5, Valid_Loss=0.347]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34mValidation Loss Improved (inf ---> 0.34703487637370817)\nModel Saved\u001b[0m\n\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 752/752 [10:42<00:00,  1.17it/s, Epoch=2, LR=1.02e-6, Train_Loss=0.358]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:48<00:00,  1.97it/s, Epoch=2, LR=1.02e-6, Valid_Loss=0.345]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34mValidation Loss Improved (0.34703487637370817 ---> 0.3447379459379837)\nModel Saved\u001b[0m\n\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 752/752 [10:42<00:00,  1.17it/s, Epoch=3, LR=4.86e-5, Train_Loss=0.351]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:48<00:00,  1.96it/s, Epoch=3, LR=4.86e-5, Valid_Loss=0.414]\n","output_type":"stream"},{"name":"stdout","text":"\nTraining complete in 0h 34m 37s\nBest Loss: 0.3447\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br/>Waiting for W&B process to finish, PID 98... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>‚ñà‚ñÉ‚ñÅ</td></tr><tr><td>Valid Loss</td><td>‚ñÅ‚ñÅ‚ñà</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Loss</td><td>0.34474</td></tr><tr><td>Train Loss</td><td>0.35094</td></tr><tr><td>Valid Loss</td><td>0.41403</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">k5nu8k69390a-fold-0</strong>: <a href=\"https://wandb.ai/dchanda/Jigsaw/runs/2qtiyn5b\" target=\"_blank\">https://wandb.ai/dchanda/Jigsaw/runs/2qtiyn5b</a><br/>\nFind logs at: <code>./wandb/run-20211109_054731-2qtiyn5b/logs</code><br/>\n"},"metadata":{}},{"name":"stdout","text":"\n\u001b[33m====== Fold: 1 ======\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                    Syncing run <strong><a href=\"https://wandb.ai/dchanda/Jigsaw/runs/3kbn6ouw\" target=\"_blank\">k5nu8k69390a-fold-1</a></strong> to <a href=\"https://wandb.ai/dchanda/Jigsaw\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "},"metadata":{}},{"name":"stdout","text":"[INFO] Using GPU: Tesla P100-PCIE-16GB\n\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 752/752 [10:40<00:00,  1.17it/s, Epoch=1, LR=5.11e-5, Train_Loss=0.366]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:48<00:00,  1.97it/s, Epoch=1, LR=5.11e-5, Valid_Loss=0.337]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34mValidation Loss Improved (inf ---> 0.3367958312135961)\nModel Saved\u001b[0m\n\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 752/752 [10:41<00:00,  1.17it/s, Epoch=2, LR=1.02e-6, Train_Loss=0.351]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:48<00:00,  1.97it/s, Epoch=2, LR=1.02e-6, Valid_Loss=0.334]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34mValidation Loss Improved (0.3367958312135961 ---> 0.33438666955215596)\nModel Saved\u001b[0m\n\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 752/752 [10:41<00:00,  1.17it/s, Epoch=3, LR=4.86e-5, Train_Loss=0.324]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:48<00:00,  1.97it/s, Epoch=3, LR=4.86e-5, Valid_Loss=0.343]\n","output_type":"stream"},{"name":"stdout","text":"\nTraining complete in 0h 34m 34s\nBest Loss: 0.3344\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br/>Waiting for W&B process to finish, PID 1517... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>‚ñà‚ñÖ‚ñÅ</td></tr><tr><td>Valid Loss</td><td>‚ñÉ‚ñÅ‚ñà</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Loss</td><td>0.33439</td></tr><tr><td>Train Loss</td><td>0.32449</td></tr><tr><td>Valid Loss</td><td>0.3433</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">k5nu8k69390a-fold-1</strong>: <a href=\"https://wandb.ai/dchanda/Jigsaw/runs/3kbn6ouw\" target=\"_blank\">https://wandb.ai/dchanda/Jigsaw/runs/3kbn6ouw</a><br/>\nFind logs at: <code>./wandb/run-20211109_062249-3kbn6ouw/logs</code><br/>\n"},"metadata":{}},{"name":"stdout","text":"\n\u001b[33m====== Fold: 2 ======\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                    Syncing run <strong><a href=\"https://wandb.ai/dchanda/Jigsaw/runs/2c9t05fj\" target=\"_blank\">k5nu8k69390a-fold-2</a></strong> to <a href=\"https://wandb.ai/dchanda/Jigsaw\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "},"metadata":{}},{"name":"stdout","text":"[INFO] Using GPU: Tesla P100-PCIE-16GB\n\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 752/752 [10:42<00:00,  1.17it/s, Epoch=1, LR=5.11e-5, Train_Loss=0.362]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:48<00:00,  1.97it/s, Epoch=1, LR=5.11e-5, Valid_Loss=0.356]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34mValidation Loss Improved (inf ---> 0.35593599433401657)\nModel Saved\u001b[0m\n\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 752/752 [10:42<00:00,  1.17it/s, Epoch=2, LR=1.02e-6, Train_Loss=0.354]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:48<00:00,  1.97it/s, Epoch=2, LR=1.02e-6, Valid_Loss=0.342]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34mValidation Loss Improved (0.35593599433401657 ---> 0.3418647890984121)\nModel Saved\u001b[0m\n\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 752/752 [10:41<00:00,  1.17it/s, Epoch=3, LR=4.86e-5, Train_Loss=0.328]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:48<00:00,  1.96it/s, Epoch=3, LR=4.86e-5, Valid_Loss=0.343]\n","output_type":"stream"},{"name":"stdout","text":"\nTraining complete in 0h 34m 36s\nBest Loss: 0.3419\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br/>Waiting for W&B process to finish, PID 2905... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>‚ñà‚ñÜ‚ñÅ</td></tr><tr><td>Valid Loss</td><td>‚ñà‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Loss</td><td>0.34186</td></tr><tr><td>Train Loss</td><td>0.32772</td></tr><tr><td>Valid Loss</td><td>0.34267</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">k5nu8k69390a-fold-2</strong>: <a href=\"https://wandb.ai/dchanda/Jigsaw/runs/2c9t05fj\" target=\"_blank\">https://wandb.ai/dchanda/Jigsaw/runs/2c9t05fj</a><br/>\nFind logs at: <code>./wandb/run-20211109_065738-2c9t05fj/logs</code><br/>\n"},"metadata":{}},{"name":"stdout","text":"\n\u001b[33m====== Fold: 3 ======\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                    Syncing run <strong><a href=\"https://wandb.ai/dchanda/Jigsaw/runs/129a7ygj\" target=\"_blank\">k5nu8k69390a-fold-3</a></strong> to <a href=\"https://wandb.ai/dchanda/Jigsaw\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "},"metadata":{}},{"name":"stdout","text":"[INFO] Using GPU: Tesla P100-PCIE-16GB\n\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 752/752 [10:44<00:00,  1.17it/s, Epoch=1, LR=5.11e-5, Train_Loss=0.363]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:48<00:00,  1.96it/s, Epoch=1, LR=5.11e-5, Valid_Loss=0.352]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34mValidation Loss Improved (inf ---> 0.3516482973297734)\nModel Saved\u001b[0m\n\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 752/752 [10:41<00:00,  1.17it/s, Epoch=2, LR=1.02e-6, Train_Loss=0.351]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:48<00:00,  1.96it/s, Epoch=2, LR=1.02e-6, Valid_Loss=0.34] \n","output_type":"stream"},{"name":"stdout","text":"\u001b[34mValidation Loss Improved (0.3516482973297734 ---> 0.3395050928934609)\nModel Saved\u001b[0m\n\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 752/752 [10:40<00:00,  1.17it/s, Epoch=3, LR=4.86e-5, Train_Loss=0.325]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:48<00:00,  1.97it/s, Epoch=3, LR=4.86e-5, Valid_Loss=0.347]\n","output_type":"stream"},{"name":"stdout","text":"\nTraining complete in 0h 34m 38s\nBest Loss: 0.3395\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br/>Waiting for W&B process to finish, PID 4302... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>‚ñà‚ñÜ‚ñÅ</td></tr><tr><td>Valid Loss</td><td>‚ñà‚ñÅ‚ñÜ</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Loss</td><td>0.33951</td></tr><tr><td>Train Loss</td><td>0.32539</td></tr><tr><td>Valid Loss</td><td>0.34738</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">k5nu8k69390a-fold-3</strong>: <a href=\"https://wandb.ai/dchanda/Jigsaw/runs/129a7ygj\" target=\"_blank\">https://wandb.ai/dchanda/Jigsaw/runs/129a7ygj</a><br/>\nFind logs at: <code>./wandb/run-20211109_073229-129a7ygj/logs</code><br/>\n"},"metadata":{}},{"name":"stdout","text":"\n\u001b[33m====== Fold: 4 ======\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                    Syncing run <strong><a href=\"https://wandb.ai/dchanda/Jigsaw/runs/2kocz0r7\" target=\"_blank\">k5nu8k69390a-fold-4</a></strong> to <a href=\"https://wandb.ai/dchanda/Jigsaw\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "},"metadata":{}},{"name":"stdout","text":"[INFO] Using GPU: Tesla P100-PCIE-16GB\n\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 752/752 [10:40<00:00,  1.17it/s, Epoch=1, LR=5.11e-5, Train_Loss=0.364]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:48<00:00,  1.97it/s, Epoch=1, LR=5.11e-5, Valid_Loss=0.35] \n","output_type":"stream"},{"name":"stdout","text":"\u001b[34mValidation Loss Improved (inf ---> 0.35034300633649695)\nModel Saved\u001b[0m\n\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 752/752 [10:41<00:00,  1.17it/s, Epoch=2, LR=1.02e-6, Train_Loss=0.347]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:48<00:00,  1.97it/s, Epoch=2, LR=1.02e-6, Valid_Loss=0.34] \n","output_type":"stream"},{"name":"stdout","text":"\u001b[34mValidation Loss Improved (0.35034300633649695 ---> 0.34013118115132324)\nModel Saved\u001b[0m\n\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 752/752 [10:41<00:00,  1.17it/s, Epoch=3, LR=4.86e-5, Train_Loss=0.347]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:48<00:00,  1.96it/s, Epoch=3, LR=4.86e-5, Valid_Loss=0.35] \n","output_type":"stream"},{"name":"stdout","text":"\nTraining complete in 0h 34m 33s\nBest Loss: 0.3401\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br/>Waiting for W&B process to finish, PID 5756... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>‚ñà‚ñÅ‚ñÅ</td></tr><tr><td>Valid Loss</td><td>‚ñà‚ñÅ‚ñà</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Loss</td><td>0.34013</td></tr><tr><td>Train Loss</td><td>0.3468</td></tr><tr><td>Valid Loss</td><td>0.34972</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">k5nu8k69390a-fold-4</strong>: <a href=\"https://wandb.ai/dchanda/Jigsaw/runs/2kocz0r7\" target=\"_blank\">https://wandb.ai/dchanda/Jigsaw/runs/2kocz0r7</a><br/>\nFind logs at: <code>./wandb/run-20211109_080721-2kocz0r7/logs</code><br/>\n"},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Visualizations</h1></span>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\"><a href=\"https://wandb.ai/dchanda/Jigsaw\">View the Complete Dashboard Here ‚Æï</a></span>","metadata":{}},{"cell_type":"markdown","source":"![](https://i.imgur.com/TSIUdfS.jpg)","metadata":{}},{"cell_type":"markdown","source":"![Upvote!](https://img.shields.io/badge/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)","metadata":{}}]}