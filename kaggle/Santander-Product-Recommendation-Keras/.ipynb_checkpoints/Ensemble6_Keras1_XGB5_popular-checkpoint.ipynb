{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import xgboost as xgb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "matplotlib.rcParams['figure.figsize'] = (8, 6)\n",
    "\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Merge\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "import time\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demographic_cols = ['ncodpers','fecha_alta','ind_empleado','pais_residencia','sexo','age','ind_nuevo','antiguedad','indrel',\n",
    " 'indrel_1mes','tiprel_1mes','indresi','indext','conyuemp','canal_entrada','indfall',\n",
    " 'tipodom','cod_prov','ind_actividad_cliente','renta','segmento']\n",
    "\n",
    "notuse = [\"ult_fec_cli_1t\",\"nomprov\",'fecha_dato']\n",
    "\n",
    "product_col = [\n",
    " 'ind_ahor_fin_ult1','ind_aval_fin_ult1','ind_cco_fin_ult1','ind_cder_fin_ult1','ind_cno_fin_ult1','ind_ctju_fin_ult1',\n",
    " 'ind_ctma_fin_ult1','ind_ctop_fin_ult1','ind_ctpp_fin_ult1','ind_deco_fin_ult1','ind_deme_fin_ult1',\n",
    " 'ind_dela_fin_ult1','ind_ecue_fin_ult1','ind_fond_fin_ult1','ind_hip_fin_ult1','ind_plan_fin_ult1',\n",
    " 'ind_pres_fin_ult1','ind_reca_fin_ult1','ind_tjcr_fin_ult1','ind_valo_fin_ult1','ind_viv_fin_ult1','ind_nomina_ult1',\n",
    " 'ind_nom_pens_ult1','ind_recibo_ult1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/r/.local/lib/python3.4/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (4,7,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('cleaned_data/DataMulticlass_6_withpast2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/r/.local/lib/python3.4/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('cleaned_data/TestSet_withpast3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unneccessary column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_column = ['ind_nuevo','indrel','indresi','indfall','tipodom','ind_empleado','pais_residencia','indrel_1mes','indext','conyuemp','fecha_alta','tiprel_1mes']\n",
    "\n",
    "df_train.drop(drop_column, axis=1, inplace = True)\n",
    "df_test.drop(drop_column, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add missing income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test[\"renta\"]   = pd.to_numeric(df_test[\"renta\"], errors=\"coerce\")\n",
    "unique_prov = df_test[df_test.cod_prov.notnull()].cod_prov.unique()\n",
    "grouped = df_test.groupby(\"cod_prov\")[\"renta\"].median()\n",
    "\n",
    "def impute_renta(df):\n",
    "    df[\"renta\"]   = pd.to_numeric(df[\"renta\"], errors=\"coerce\")       \n",
    "    for cod in unique_prov:\n",
    "        df.loc[df['cod_prov']==cod,['renta']] = df.loc[df['cod_prov']==cod,['renta']].fillna({'renta':grouped[cod]}).values\n",
    "    df.renta.fillna(df_test[\"renta\"].median(), inplace=True)\n",
    "    \n",
    "impute_renta(df_train)\n",
    "impute_renta(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def drop_na(df):\n",
    "    df.dropna(axis = 0, subset = ['ind_actividad_cliente'], inplace = True) \n",
    "    \n",
    "drop_na(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert and make dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These column are categories feature, I'll transform them using get_dummy\n",
    "dummy_col = ['sexo','canal_entrada','cod_prov','segmento']\n",
    "dummy_col_select = ['canal_entrada','cod_prov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "limit = int(0.05 * len(df_train.index))\n",
    "use_dummy_col = {}\n",
    "\n",
    "for col in dummy_col_select:\n",
    "    trainlist = df_train[col].value_counts()\n",
    "    use_dummy_col[col] = []\n",
    "    for i,item in enumerate(trainlist):\n",
    "        if item > limit:\n",
    "            use_dummy_col[col].append(df_train[col].value_counts().index[i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dummy(df):\n",
    "    for col in dummy_col_select:\n",
    "        for item in df[col].unique(): \n",
    "            if item not in use_dummy_col[col]:\n",
    "                row_index = df[col] == item\n",
    "                df.loc[row_index,col] = np.nan\n",
    "    return pd.get_dummies(df, prefix=dummy_col, columns = dummy_col)\n",
    "    \n",
    "df_train = get_dummy(df_train)\n",
    "df_test = get_dummy(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_quantitative_param(df):\n",
    "    df[\"age\"]   = pd.to_numeric(df[\"age\"], errors=\"coerce\")\n",
    "    max_age = 80 \n",
    "    log_max_age = np.log(max_age) \n",
    "    square_max_age  = np.square(max_age)\n",
    "    df[\"age\"]   = df['age'].apply(lambda x: min(x ,max_age))\n",
    "    df[\"log_age\"]   = df['age'].apply(lambda x: round(np.log10(x+1)/log_max_age, 6))\n",
    "    df[\"square_age\"]   = df['age'].apply(lambda x: round(np.square(x)/square_max_age, 6))\n",
    "    df[\"age\"]   = df['age'].apply(lambda x: round( x/max_age, 6))\n",
    "    \n",
    "    max_renta = 1.0e6\n",
    "    log_max_renta = np.log(max_renta) \n",
    "    square_max_renta  = np.square(max_renta)\n",
    "    df[\"renta\"]   = df['renta'].apply(lambda x: min(x ,max_renta))\n",
    "    df[\"log_renta\"]   = df['renta'].apply(lambda x: round(np.log10(x+1)/log_max_renta, 6))\n",
    "    df[\"square_renta\"]   = df['renta'].apply(lambda x: round(np.square(x)/square_max_renta, 6))\n",
    "    df[\"renta\"]   = df['renta'].apply(lambda x: round( x/max_renta, 6))\n",
    "    \n",
    "    df[\"antiguedad\"]   = pd.to_numeric(df[\"antiguedad\"], errors=\"coerce\")\n",
    "    df[\"antiguedad\"] = df[\"antiguedad\"].replace(-999999, df['antiguedad'].median())\n",
    "    max_antigue = 256\n",
    "    log_max_antigue = np.log(max_antigue) \n",
    "    square_max_antigue  = np.square(max_antigue)\n",
    "    df[\"antiguedad\"]   = df['antiguedad'].apply(lambda x: min(x ,max_antigue))\n",
    "    df[\"log_antiguedad\"]   = df['antiguedad'].apply(lambda x: round(np.log10(x+1)/log_max_antigue, 6))\n",
    "    df[\"square_antiguedad\"]   = df['antiguedad'].apply(lambda x: round(np.square(x)/square_max_antigue, 6))\n",
    "    df[\"antiguedad\"]   = df['antiguedad'].apply(lambda x: round( x/max_antigue, 6)) \n",
    "    \n",
    "    return df      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = clean_quantitative_param(df_train)\n",
    "df_test = clean_quantitative_param(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "product_col_5 = [col for col in df_train.columns if '_ult1_5' in col]\n",
    "product_col_4 = [col for col in df_train.columns if '_ult1_4' in col]\n",
    "product_col_3 = [col for col in df_train.columns if '_ult1_3' in col]\n",
    "product_col_2 = [col for col in df_train.columns if '_ult1_2' in col]\n",
    "product_col_1 = [col for col in df_train.columns if '_ult1_1' in col]\n",
    "\n",
    "df_train['tot5'] = df_train[product_col_5].sum(axis=1)\n",
    "df_test['tot5'] = df_test[product_col_5].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in product_col[2:]:\n",
    "    df_train[col+'_past'] = (df_train[col+'_5']+df_train[col+'_4']+df_train[col+'_3']+df_train[col+'_2']+df_train[col+'_1'])/5\n",
    "    df_test[col+'_past'] = (df_test[col+'_5']+df_test[col+'_4']+df_test[col+'_3']+df_test[col+'_2']+df_test[col+'_1'])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pro in product_col[2:]:\n",
    "    df_train[pro+'_past'] = df_train[pro+'_past']*(1-df_train[pro+'_5'])\n",
    "    df_test[pro+'_past'] = df_test[pro+'_past']*(1-df_test[pro+'_5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col1 = product_col_1 + product_col_2 + product_col_3 + product_col_4 + product_col_5\n",
    "col2 = list(df_train.drop(['target','ncodpers']+col1, 1).columns.values)\n",
    "\n",
    "x_train1 = df_train[col1].as_matrix()\n",
    "x_test1 = df_test[col1].as_matrix()\n",
    "\n",
    "x_train1 = np.reshape(x_train1, (len(x_train1), 5, 22))\n",
    "x_test1 = np.reshape(x_test1, (len(x_test1), 5, 22))\n",
    "\n",
    "x_train2 = df_train[col2].as_matrix()\n",
    "x_test2 = df_test[col2].as_matrix()\n",
    "\n",
    "y_train = pd.get_dummies(df_train['target'].astype(int)).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution1d_1 (Convolution1D)  (None, 5, 60)         29100                                        \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 300)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_2 (Convolution1D)  (None, 22, 30)        780                                          \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 660)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 150)           6750                                         \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 22)            24442       merge_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 61072\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "0s - loss: 1.2040 - categorical_accuracy: 0.6137\n",
      "Epoch 2/100\n",
      "0s - loss: 1.0435 - categorical_accuracy: 0.6450\n",
      "Epoch 3/100\n",
      "0s - loss: 1.0182 - categorical_accuracy: 0.6461\n",
      "Epoch 4/100\n",
      "0s - loss: 1.0051 - categorical_accuracy: 0.6492\n",
      "Epoch 5/100\n",
      "0s - loss: 0.9964 - categorical_accuracy: 0.6500\n",
      "Epoch 6/100\n",
      "0s - loss: 0.9906 - categorical_accuracy: 0.6508\n",
      "Epoch 7/100\n",
      "0s - loss: 0.9856 - categorical_accuracy: 0.6501\n",
      "Epoch 8/100\n",
      "0s - loss: 0.9820 - categorical_accuracy: 0.6511\n",
      "Epoch 9/100\n",
      "0s - loss: 0.9786 - categorical_accuracy: 0.6534\n",
      "Epoch 10/100\n",
      "0s - loss: 0.9758 - categorical_accuracy: 0.6531\n",
      "Epoch 11/100\n",
      "0s - loss: 0.9733 - categorical_accuracy: 0.6535\n",
      "Epoch 12/100\n",
      "0s - loss: 0.9710 - categorical_accuracy: 0.6542\n",
      "Epoch 13/100\n",
      "0s - loss: 0.9688 - categorical_accuracy: 0.6537\n",
      "Epoch 14/100\n",
      "0s - loss: 0.9672 - categorical_accuracy: 0.6543\n",
      "Epoch 15/100\n",
      "0s - loss: 0.9654 - categorical_accuracy: 0.6558\n",
      "Epoch 16/100\n",
      "0s - loss: 0.9641 - categorical_accuracy: 0.6541\n",
      "Epoch 17/100\n",
      "0s - loss: 0.9626 - categorical_accuracy: 0.6545\n",
      "Epoch 18/100\n",
      "0s - loss: 0.9608 - categorical_accuracy: 0.6543\n",
      "Epoch 19/100\n",
      "0s - loss: 0.9599 - categorical_accuracy: 0.6551\n",
      "Epoch 20/100\n",
      "0s - loss: 0.9589 - categorical_accuracy: 0.6553\n",
      "Epoch 21/100\n",
      "0s - loss: 0.9577 - categorical_accuracy: 0.6545\n",
      "Epoch 22/100\n",
      "0s - loss: 0.9566 - categorical_accuracy: 0.6551\n",
      "Epoch 23/100\n",
      "0s - loss: 0.9556 - categorical_accuracy: 0.6562\n",
      "Epoch 24/100\n",
      "0s - loss: 0.9546 - categorical_accuracy: 0.6559\n",
      "Epoch 25/100\n",
      "0s - loss: 0.9537 - categorical_accuracy: 0.6569\n",
      "Epoch 26/100\n",
      "0s - loss: 0.9529 - categorical_accuracy: 0.6563\n",
      "Epoch 27/100\n",
      "0s - loss: 0.9519 - categorical_accuracy: 0.6564\n",
      "Epoch 28/100\n",
      "0s - loss: 0.9511 - categorical_accuracy: 0.6561\n",
      "Epoch 29/100\n",
      "0s - loss: 0.9506 - categorical_accuracy: 0.6569\n",
      "Epoch 30/100\n",
      "0s - loss: 0.9498 - categorical_accuracy: 0.6583\n",
      "Epoch 31/100\n",
      "0s - loss: 0.9490 - categorical_accuracy: 0.6570\n",
      "Epoch 32/100\n",
      "0s - loss: 0.9483 - categorical_accuracy: 0.6559\n",
      "Epoch 33/100\n",
      "0s - loss: 0.9476 - categorical_accuracy: 0.6582\n",
      "Epoch 34/100\n",
      "0s - loss: 0.9468 - categorical_accuracy: 0.6567\n",
      "Epoch 35/100\n",
      "0s - loss: 0.9464 - categorical_accuracy: 0.6567\n",
      "Epoch 36/100\n",
      "0s - loss: 0.9458 - categorical_accuracy: 0.6577\n",
      "Epoch 37/100\n",
      "0s - loss: 0.9454 - categorical_accuracy: 0.6570\n",
      "Epoch 38/100\n",
      "0s - loss: 0.9448 - categorical_accuracy: 0.6578\n",
      "Epoch 39/100\n",
      "0s - loss: 0.9442 - categorical_accuracy: 0.6561\n",
      "Epoch 40/100\n",
      "0s - loss: 0.9437 - categorical_accuracy: 0.6583\n",
      "Epoch 41/100\n",
      "0s - loss: 0.9431 - categorical_accuracy: 0.6585\n",
      "Epoch 42/100\n",
      "0s - loss: 0.9427 - categorical_accuracy: 0.6567\n",
      "Epoch 43/100\n",
      "0s - loss: 0.9422 - categorical_accuracy: 0.6579\n",
      "Epoch 44/100\n",
      "0s - loss: 0.9416 - categorical_accuracy: 0.6578\n",
      "Epoch 45/100\n",
      "0s - loss: 0.9412 - categorical_accuracy: 0.6588\n",
      "Epoch 46/100\n",
      "0s - loss: 0.9407 - categorical_accuracy: 0.6582\n",
      "Epoch 47/100\n",
      "0s - loss: 0.9404 - categorical_accuracy: 0.6587\n",
      "Epoch 48/100\n",
      "0s - loss: 0.9399 - categorical_accuracy: 0.6586\n",
      "Epoch 49/100\n",
      "0s - loss: 0.9396 - categorical_accuracy: 0.6586\n",
      "Epoch 50/100\n",
      "0s - loss: 0.9390 - categorical_accuracy: 0.6590\n",
      "Epoch 51/100\n",
      "0s - loss: 0.9386 - categorical_accuracy: 0.6596\n",
      "Epoch 52/100\n",
      "0s - loss: 0.9381 - categorical_accuracy: 0.6603\n",
      "Epoch 53/100\n",
      "0s - loss: 0.9378 - categorical_accuracy: 0.6585\n",
      "Epoch 54/100\n",
      "0s - loss: 0.9374 - categorical_accuracy: 0.6591\n",
      "Epoch 55/100\n",
      "0s - loss: 0.9370 - categorical_accuracy: 0.6595\n",
      "Epoch 56/100\n",
      "0s - loss: 0.9368 - categorical_accuracy: 0.6592\n",
      "Epoch 57/100\n",
      "0s - loss: 0.9362 - categorical_accuracy: 0.6597\n",
      "Epoch 58/100\n",
      "0s - loss: 0.9362 - categorical_accuracy: 0.6586\n",
      "Epoch 59/100\n",
      "0s - loss: 0.9356 - categorical_accuracy: 0.6587\n",
      "Epoch 60/100\n",
      "0s - loss: 0.9354 - categorical_accuracy: 0.6595\n",
      "Epoch 61/100\n",
      "0s - loss: 0.9352 - categorical_accuracy: 0.6594\n",
      "Epoch 62/100\n",
      "0s - loss: 0.9346 - categorical_accuracy: 0.6598\n",
      "Epoch 63/100\n",
      "0s - loss: 0.9344 - categorical_accuracy: 0.6586\n",
      "Epoch 64/100\n",
      "0s - loss: 0.9340 - categorical_accuracy: 0.6585\n",
      "Epoch 65/100\n",
      "0s - loss: 0.9339 - categorical_accuracy: 0.6587\n",
      "Epoch 66/100\n",
      "0s - loss: 0.9335 - categorical_accuracy: 0.6576\n",
      "Epoch 67/100\n",
      "0s - loss: 0.9331 - categorical_accuracy: 0.6586\n",
      "Epoch 68/100\n",
      "0s - loss: 0.9328 - categorical_accuracy: 0.6604\n",
      "Epoch 69/100\n",
      "0s - loss: 0.9325 - categorical_accuracy: 0.6604\n",
      "Epoch 70/100\n",
      "0s - loss: 0.9322 - categorical_accuracy: 0.6618\n",
      "Epoch 71/100\n",
      "0s - loss: 0.9319 - categorical_accuracy: 0.6605\n",
      "Epoch 72/100\n",
      "0s - loss: 0.9316 - categorical_accuracy: 0.6608\n",
      "Epoch 73/100\n",
      "0s - loss: 0.9313 - categorical_accuracy: 0.6597\n",
      "Epoch 74/100\n",
      "0s - loss: 0.9310 - categorical_accuracy: 0.6592\n",
      "Epoch 75/100\n",
      "0s - loss: 0.9307 - categorical_accuracy: 0.6595\n",
      "Epoch 76/100\n",
      "0s - loss: 0.9305 - categorical_accuracy: 0.6594\n",
      "Epoch 77/100\n",
      "0s - loss: 0.9302 - categorical_accuracy: 0.6598\n",
      "Epoch 78/100\n",
      "0s - loss: 0.9301 - categorical_accuracy: 0.6599\n",
      "Epoch 79/100\n",
      "0s - loss: 0.9297 - categorical_accuracy: 0.6604\n",
      "Epoch 80/100\n",
      "0s - loss: 0.9294 - categorical_accuracy: 0.6613\n",
      "Epoch 81/100\n",
      "0s - loss: 0.9292 - categorical_accuracy: 0.6608\n",
      "Epoch 82/100\n",
      "0s - loss: 0.9289 - categorical_accuracy: 0.6612\n",
      "Epoch 83/100\n",
      "0s - loss: 0.9287 - categorical_accuracy: 0.6602\n",
      "Epoch 84/100\n",
      "0s - loss: 0.9285 - categorical_accuracy: 0.6593\n",
      "Epoch 85/100\n",
      "0s - loss: 0.9282 - categorical_accuracy: 0.6608\n",
      "Epoch 86/100\n",
      "0s - loss: 0.9279 - categorical_accuracy: 0.6616\n",
      "Epoch 87/100\n",
      "0s - loss: 0.9277 - categorical_accuracy: 0.6596\n",
      "Epoch 88/100\n",
      "0s - loss: 0.9274 - categorical_accuracy: 0.6606\n",
      "Epoch 89/100\n",
      "0s - loss: 0.9271 - categorical_accuracy: 0.6611\n",
      "Epoch 90/100\n",
      "0s - loss: 0.9271 - categorical_accuracy: 0.6604\n",
      "Epoch 91/100\n",
      "0s - loss: 0.9267 - categorical_accuracy: 0.6617\n",
      "Epoch 92/100\n",
      "0s - loss: 0.9267 - categorical_accuracy: 0.6595\n",
      "Epoch 93/100\n",
      "0s - loss: 0.9262 - categorical_accuracy: 0.6611\n",
      "Epoch 94/100\n",
      "0s - loss: 0.9261 - categorical_accuracy: 0.6616\n",
      "Epoch 95/100\n",
      "0s - loss: 0.9258 - categorical_accuracy: 0.6613\n",
      "Epoch 96/100\n",
      "0s - loss: 0.9258 - categorical_accuracy: 0.6611\n",
      "Epoch 97/100\n",
      "0s - loss: 0.9255 - categorical_accuracy: 0.6603\n",
      "Epoch 98/100\n",
      "0s - loss: 0.9253 - categorical_accuracy: 0.6608\n",
      "Epoch 99/100\n",
      "0s - loss: 0.9251 - categorical_accuracy: 0.6595\n",
      "Epoch 100/100\n",
      "0s - loss: 0.9247 - categorical_accuracy: 0.6619\n"
     ]
    }
   ],
   "source": [
    "id_preds1 = defaultdict(list)\n",
    "ids = df_test['ncodpers'].values\n",
    "  \n",
    "### product-wise\n",
    "model1 = Sequential()\n",
    "model1.add(Convolution1D(60, 22, border_mode='same', input_shape=(5, 22),activation = 'relu'))\n",
    "model1.add(Flatten())\n",
    "\n",
    "# time-wise\n",
    "model2 = Sequential()\n",
    "model2.add(Convolution1D(30, 5, border_mode='same', input_shape=(22, 5),activation = 'relu'))\n",
    "model2.add(Flatten())\n",
    "\n",
    "# domegraphic-wise\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(150, input_dim=len(col2), init='uniform', activation='relu'))\n",
    "\n",
    "merged = Merge([model1,model2,model3], mode='concat')\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "final_model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "final_model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['categorical_accuracy'])\n",
    "print(final_model.summary())\n",
    "\n",
    "history = final_model.fit([x_train1, x_train1.transpose((0, 2, 1)), x_train2], y_train, nb_epoch=100, batch_size=100, verbose = 2)\n",
    "        \n",
    "p_test = final_model.predict([x_test1, x_test1.transpose((0, 2, 1)), x_test2])\n",
    "        \n",
    "for id, p in zip(ids, p_test):\n",
    "    #id_preds[id] = list(p)\n",
    "    id_preds1[id] = [0,0] + list(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, reg =100, colsample_bytree=0.9, max_depth= 6, eta=0.1, min_child_weight=2, subsample=0.9, num_rounds=150):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['seed'] = 0\n",
    "    param['silent'] = 0\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['booster'] = 'gbtree'\n",
    "    param['num_class'] = 22\n",
    "    param['reg_lambda'] = reg\n",
    "    param['colsample_bytree'] = colsample_bytree\n",
    "    param['max_depth'] = max_depth \n",
    "    param['eta'] = eta\n",
    "    param['min_child_weight'] = min_child_weight\n",
    "    param['subsample'] = subsample\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    progress = dict()\n",
    "    plst = list(param.items())\n",
    "    \n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "    watchlist  = [(xgtrain,'train')]\n",
    "    model = xgb.train(plst, xgtrain, num_rounds, watchlist, evals_result=progress)\n",
    "    \n",
    "    #xgb.cv(param, xgtrain, num_rounds, nfold=3,\n",
    "    #   metrics={'mlogloss'}, seed = 0,\n",
    "    #   callbacks=[xgb.callback.print_evaluation(show_stdv=True)])\n",
    "    \n",
    "    return (model, progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.86693\n",
      "[1]\ttrain-mlogloss:2.70496\n",
      "[2]\ttrain-mlogloss:2.56824\n",
      "[3]\ttrain-mlogloss:2.45361\n",
      "[4]\ttrain-mlogloss:2.35473\n",
      "[5]\ttrain-mlogloss:2.2679\n",
      "[6]\ttrain-mlogloss:2.19099\n",
      "[7]\ttrain-mlogloss:2.12172\n",
      "[8]\ttrain-mlogloss:2.05876\n",
      "[9]\ttrain-mlogloss:2.00139\n",
      "[10]\ttrain-mlogloss:1.94895\n",
      "[11]\ttrain-mlogloss:1.90034\n",
      "[12]\ttrain-mlogloss:1.85543\n",
      "[13]\ttrain-mlogloss:1.81435\n",
      "[14]\ttrain-mlogloss:1.77524\n",
      "[15]\ttrain-mlogloss:1.73878\n",
      "[16]\ttrain-mlogloss:1.70462\n",
      "[17]\ttrain-mlogloss:1.67226\n",
      "[18]\ttrain-mlogloss:1.64269\n",
      "[19]\ttrain-mlogloss:1.61407\n",
      "[20]\ttrain-mlogloss:1.58713\n",
      "[21]\ttrain-mlogloss:1.56169\n",
      "[22]\ttrain-mlogloss:1.53759\n",
      "[23]\ttrain-mlogloss:1.51459\n",
      "[24]\ttrain-mlogloss:1.49293\n",
      "[25]\ttrain-mlogloss:1.47238\n",
      "[26]\ttrain-mlogloss:1.45292\n",
      "[27]\ttrain-mlogloss:1.43423\n",
      "[28]\ttrain-mlogloss:1.41649\n",
      "[29]\ttrain-mlogloss:1.39954\n",
      "[30]\ttrain-mlogloss:1.38347\n",
      "[31]\ttrain-mlogloss:1.36807\n",
      "[32]\ttrain-mlogloss:1.35332\n",
      "[33]\ttrain-mlogloss:1.33952\n",
      "[34]\ttrain-mlogloss:1.32621\n",
      "[35]\ttrain-mlogloss:1.31342\n",
      "[36]\ttrain-mlogloss:1.30115\n",
      "[37]\ttrain-mlogloss:1.28926\n",
      "[38]\ttrain-mlogloss:1.27799\n",
      "[39]\ttrain-mlogloss:1.26725\n",
      "[40]\ttrain-mlogloss:1.25686\n",
      "[41]\ttrain-mlogloss:1.24694\n",
      "[42]\ttrain-mlogloss:1.23743\n",
      "[43]\ttrain-mlogloss:1.2282\n",
      "[44]\ttrain-mlogloss:1.21939\n",
      "[45]\ttrain-mlogloss:1.21095\n",
      "[46]\ttrain-mlogloss:1.20287\n",
      "[47]\ttrain-mlogloss:1.19503\n",
      "[48]\ttrain-mlogloss:1.18753\n",
      "[49]\ttrain-mlogloss:1.18039\n",
      "[50]\ttrain-mlogloss:1.17337\n",
      "[51]\ttrain-mlogloss:1.1667\n",
      "[52]\ttrain-mlogloss:1.16014\n",
      "[53]\ttrain-mlogloss:1.15396\n",
      "[54]\ttrain-mlogloss:1.14796\n",
      "[55]\ttrain-mlogloss:1.14225\n",
      "[56]\ttrain-mlogloss:1.13658\n",
      "[57]\ttrain-mlogloss:1.13126\n",
      "[58]\ttrain-mlogloss:1.12602\n",
      "[59]\ttrain-mlogloss:1.12097\n",
      "[60]\ttrain-mlogloss:1.11599\n",
      "[61]\ttrain-mlogloss:1.11132\n",
      "[62]\ttrain-mlogloss:1.10671\n",
      "[63]\ttrain-mlogloss:1.10228\n",
      "[64]\ttrain-mlogloss:1.098\n",
      "[65]\ttrain-mlogloss:1.09386\n",
      "[66]\ttrain-mlogloss:1.0899\n",
      "[67]\ttrain-mlogloss:1.08605\n",
      "[68]\ttrain-mlogloss:1.08229\n",
      "[69]\ttrain-mlogloss:1.07864\n",
      "[70]\ttrain-mlogloss:1.0751\n",
      "[71]\ttrain-mlogloss:1.07167\n",
      "[72]\ttrain-mlogloss:1.06834\n",
      "[73]\ttrain-mlogloss:1.0651\n",
      "[74]\ttrain-mlogloss:1.06197\n",
      "[75]\ttrain-mlogloss:1.059\n",
      "[76]\ttrain-mlogloss:1.05602\n",
      "[77]\ttrain-mlogloss:1.05314\n",
      "[78]\ttrain-mlogloss:1.05029\n",
      "[79]\ttrain-mlogloss:1.04758\n",
      "[80]\ttrain-mlogloss:1.04499\n",
      "[81]\ttrain-mlogloss:1.04249\n",
      "[82]\ttrain-mlogloss:1.03999\n",
      "[83]\ttrain-mlogloss:1.03761\n",
      "[84]\ttrain-mlogloss:1.03523\n",
      "[85]\ttrain-mlogloss:1.03293\n",
      "[86]\ttrain-mlogloss:1.0307\n",
      "[87]\ttrain-mlogloss:1.02849\n",
      "[88]\ttrain-mlogloss:1.02641\n",
      "[89]\ttrain-mlogloss:1.02436\n",
      "[90]\ttrain-mlogloss:1.02236\n",
      "[91]\ttrain-mlogloss:1.02037\n",
      "[92]\ttrain-mlogloss:1.01843\n",
      "[93]\ttrain-mlogloss:1.01658\n",
      "[94]\ttrain-mlogloss:1.0147\n",
      "[95]\ttrain-mlogloss:1.01292\n",
      "[96]\ttrain-mlogloss:1.01127\n",
      "[97]\ttrain-mlogloss:1.0095\n",
      "[98]\ttrain-mlogloss:1.00793\n",
      "[99]\ttrain-mlogloss:1.00634\n",
      "[100]\ttrain-mlogloss:1.00478\n",
      "[101]\ttrain-mlogloss:1.00323\n",
      "[102]\ttrain-mlogloss:1.00167\n",
      "[103]\ttrain-mlogloss:1.00024\n",
      "[104]\ttrain-mlogloss:0.998776\n",
      "[105]\ttrain-mlogloss:0.997359\n",
      "[106]\ttrain-mlogloss:0.995996\n",
      "[107]\ttrain-mlogloss:0.994668\n",
      "[108]\ttrain-mlogloss:0.993291\n",
      "[109]\ttrain-mlogloss:0.991985\n",
      "[110]\ttrain-mlogloss:0.990661\n",
      "[111]\ttrain-mlogloss:0.989394\n",
      "[112]\ttrain-mlogloss:0.988175\n",
      "[113]\ttrain-mlogloss:0.987008\n",
      "[114]\ttrain-mlogloss:0.985818\n",
      "[115]\ttrain-mlogloss:0.984635\n",
      "[116]\ttrain-mlogloss:0.983505\n",
      "[117]\ttrain-mlogloss:0.982379\n",
      "[118]\ttrain-mlogloss:0.981261\n",
      "[119]\ttrain-mlogloss:0.980172\n",
      "[120]\ttrain-mlogloss:0.979137\n",
      "[121]\ttrain-mlogloss:0.978158\n",
      "[122]\ttrain-mlogloss:0.977123\n",
      "[123]\ttrain-mlogloss:0.976143\n",
      "[124]\ttrain-mlogloss:0.975095\n",
      "[125]\ttrain-mlogloss:0.974101\n",
      "[126]\ttrain-mlogloss:0.97321\n",
      "[127]\ttrain-mlogloss:0.97231\n",
      "[128]\ttrain-mlogloss:0.97144\n",
      "[129]\ttrain-mlogloss:0.97052\n",
      "[130]\ttrain-mlogloss:0.969665\n",
      "[131]\ttrain-mlogloss:0.968802\n",
      "[132]\ttrain-mlogloss:0.967962\n",
      "[133]\ttrain-mlogloss:0.967157\n",
      "[134]\ttrain-mlogloss:0.966386\n",
      "[135]\ttrain-mlogloss:0.96557\n",
      "[136]\ttrain-mlogloss:0.964821\n",
      "[137]\ttrain-mlogloss:0.964096\n",
      "[138]\ttrain-mlogloss:0.963338\n",
      "[139]\ttrain-mlogloss:0.96261\n",
      "[140]\ttrain-mlogloss:0.961901\n",
      "[141]\ttrain-mlogloss:0.961088\n",
      "[142]\ttrain-mlogloss:0.960444\n",
      "[143]\ttrain-mlogloss:0.959771\n",
      "[144]\ttrain-mlogloss:0.959097\n",
      "[145]\ttrain-mlogloss:0.958472\n",
      "[146]\ttrain-mlogloss:0.957793\n",
      "[147]\ttrain-mlogloss:0.957103\n",
      "[148]\ttrain-mlogloss:0.956472\n",
      "[149]\ttrain-mlogloss:0.955812\n",
      "[150]\ttrain-mlogloss:0.955188\n",
      "[151]\ttrain-mlogloss:0.954525\n",
      "[152]\ttrain-mlogloss:0.953903\n",
      "[153]\ttrain-mlogloss:0.953323\n",
      "[154]\ttrain-mlogloss:0.952706\n",
      "[155]\ttrain-mlogloss:0.952114\n",
      "[156]\ttrain-mlogloss:0.951537\n",
      "[157]\ttrain-mlogloss:0.950926\n",
      "[158]\ttrain-mlogloss:0.950394\n",
      "[159]\ttrain-mlogloss:0.949881\n",
      "[160]\ttrain-mlogloss:0.949305\n",
      "[161]\ttrain-mlogloss:0.94876\n",
      "[162]\ttrain-mlogloss:0.948262\n",
      "[163]\ttrain-mlogloss:0.9477\n",
      "[164]\ttrain-mlogloss:0.947155\n",
      "[165]\ttrain-mlogloss:0.946674\n",
      "[166]\ttrain-mlogloss:0.946176\n",
      "[167]\ttrain-mlogloss:0.945689\n",
      "[168]\ttrain-mlogloss:0.945158\n",
      "[169]\ttrain-mlogloss:0.944686\n",
      "[170]\ttrain-mlogloss:0.944242\n",
      "[171]\ttrain-mlogloss:0.943733\n",
      "[172]\ttrain-mlogloss:0.943217\n",
      "[173]\ttrain-mlogloss:0.942698\n",
      "[174]\ttrain-mlogloss:0.942259\n",
      "[175]\ttrain-mlogloss:0.941842\n",
      "[176]\ttrain-mlogloss:0.941409\n",
      "[177]\ttrain-mlogloss:0.940953\n",
      "[178]\ttrain-mlogloss:0.9405\n",
      "[179]\ttrain-mlogloss:0.940095\n",
      "[180]\ttrain-mlogloss:0.939642\n",
      "[181]\ttrain-mlogloss:0.939189\n",
      "[182]\ttrain-mlogloss:0.938773\n",
      "[183]\ttrain-mlogloss:0.938371\n",
      "[184]\ttrain-mlogloss:0.937919\n",
      "[185]\ttrain-mlogloss:0.937473\n",
      "[186]\ttrain-mlogloss:0.937064\n",
      "[187]\ttrain-mlogloss:0.936654\n",
      "[188]\ttrain-mlogloss:0.936252\n",
      "[189]\ttrain-mlogloss:0.935849\n",
      "[190]\ttrain-mlogloss:0.935411\n",
      "[191]\ttrain-mlogloss:0.935018\n",
      "[192]\ttrain-mlogloss:0.934577\n",
      "[193]\ttrain-mlogloss:0.934206\n"
     ]
    }
   ],
   "source": [
    "cols = list(df_train.drop(['target','ncodpers'], 1).columns.values)\n",
    "#cols= selected_col\n",
    "\n",
    "id_preds2 = defaultdict(list)\n",
    "ids = df_test['ncodpers'].values\n",
    "\n",
    "# predict model \n",
    "y_train = df_train['target']\n",
    "x_train = df_train[cols]\n",
    "    \n",
    "(clf, progress) = runXGB(x_train, y_train, reg =50, eta=0.05,min_child_weight=10, num_rounds=194)\n",
    "          \n",
    "x_test = df_test[cols]\n",
    "x_test = x_test.fillna(0) \n",
    "        \n",
    "d_test = xgb.DMatrix(x_test)\n",
    "p_test = clf.predict(d_test)\n",
    "        \n",
    "for id, p in zip(ids, p_test):\n",
    "    #id_preds[id] = list(p)\n",
    "    id_preds2[id] = [0,0] + list(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#feature_imp = clf.get_fscore()\n",
    "\n",
    "#sorted_feature = sorted(feature_imp.items(), key=operator.itemgetter(1), reverse=True)\n",
    "#selected_col = [a for a,b in sorted_feature[:101]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Product Ranking: Recent 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "product_list = df_test[product_col_5].sum(axis=0)/(df_test[product_col_5].sum(axis=0).sum())\n",
    "\n",
    "id_preds5 = {}\n",
    "for row in df_test.values:\n",
    "    id = row[0]\n",
    "    id_preds5[id] = [0,0]+ list(product_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fractionKeras = 0.3\n",
    "fractionXGB = 0.7\n",
    "fractionRanking = 0.1\n",
    "id_preds_combined = {}\n",
    "\n",
    "for uid, p in id_preds1.items():\n",
    "    id_preds_combined[uid] = fractionKeras*np.asarray(id_preds1[uid]) + fractionXGB*np.asarray(id_preds2[uid]) + fractionRanking*np.asarray(id_preds5[uid])\n",
    "    \n",
    "id_preds = id_preds_combined    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_recent =  pd.read_csv('cleaned_data/df_recent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv('input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check if customer already have each product or not. \n",
    "already_active = {}\n",
    "for row in df_recent.values:\n",
    "    row = list(row)\n",
    "    id = row.pop(0)\n",
    "    active = [c[0] for c in zip(tuple(product_col), row) if c[1] > 0]\n",
    "    already_active[id] = active\n",
    "\n",
    "# add 7 products(that user don't have yet), higher probability first -> train_pred   \n",
    "train_preds = {}\n",
    "for id, p in id_preds.items():\n",
    "    preds = [i[0] for i in sorted([i for i in zip(tuple(product_col), p) if i[0] not in already_active[id]],\n",
    "                                  key=lambda i:i [1], \n",
    "                                  reverse=True)[:7]]\n",
    "    train_preds[id] = preds\n",
    "    \n",
    "test_preds = []\n",
    "for row in sample.values:\n",
    "    id = row[0]\n",
    "    p = train_preds[id]\n",
    "    test_preds.append(' '.join(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(929615, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample['added_products'] = test_preds\n",
    "sample.to_csv('output/Ensemble12.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Validation part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(time.strftime('%a %H:%M:%S'))\n",
    "\n",
    "cols = list(df_train.drop(['target','ncodpers'], 1).columns.values)\n",
    "\n",
    "# id_preds1 = defaultdict(list)\n",
    "# ids = df_test['ncodpers'].values\n",
    "\n",
    "# predict model \n",
    "y_train = pd.get_dummies(df_train['target'].astype(int))\n",
    "x_train = df_train[cols]\n",
    "    \n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(150, input_dim=len(cols), init='uniform', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(Dense(50, init='uniform', activation='relu'))\n",
    "model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "history = model.fit(x_train.as_matrix(), y_train.as_matrix(), validation_split=0.2, nb_epoch=170, batch_size=100, verbose = 2)\n",
    "\n",
    "print(time.strftime('%a %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history.history['val_loss']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(150, input_dim=len(cols), init='uniform', activation='relu'))\n",
    "model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['categorical_accuracy'])\n",
    "history = model.fit(x_train.as_matrix(), y_train.as_matrix(), validation_split=0.2, nb_epoch=150, batch_size=10, verbose = 0)\n",
    "\n",
    "val-loss: 0.997746453382"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.add(Dense(150, input_dim=len(cols), init='uniform', activation='relu'))\n",
    "model.add(Dense(50, init='uniform', activation='relu'))\n",
    "model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
    "\n",
    "val-loss: 1.01431617973"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(150, input_dim=len(cols), init='uniform', activation='relu'))\n",
    "model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
    "history = model.fit(x_train.as_matrix(), y_train.as_matrix(), validation_split=0.2, nb_epoch=130, batch_size=100, verbose = 10)\n",
    "\n",
    "val-loss: 0.99613085681124991"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(150, input_dim=len(cols), init='uniform', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
    "history = model.fit(x_train.as_matrix(), y_train.as_matrix(), validation_split=0.2, nb_epoch=170, batch_size=100, verbose = 2)\n",
    "\n",
    "val-loss: 0.99773336471464702"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col1 = product_col_1 + product_col_2 + product_col_3 + product_col_4 + product_col_5\n",
    "col2 = list(df_train.drop(['target','ncodpers']+col1, 1).columns.values)\n",
    "\n",
    "x_train1 = df_train[col1].as_matrix()\n",
    "x_test1 = df_test[col1].as_matrix()\n",
    "\n",
    "x_train1 = np.reshape(x_train1, (len(x_train1), 5, 22))\n",
    "x_test1 = np.reshape(x_test1, (len(x_test1), 5, 22))\n",
    "\n",
    "x_train2 = df_train[col2].as_matrix()\n",
    "x_test2 = df_test[col2].as_matrix()\n",
    "\n",
    "y_train = pd.get_dummies(df_train['target'].astype(int)).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(time.strftime('%a %H:%M:%S'))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape=(5, 22), dropout_W=0.0, dropout_U=0.4))\n",
    "model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['categorical_accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "history = model.fit(x_train1, y_train, validation_split=0.2, nb_epoch=200, batch_size=100, verbose = 2)\n",
    "\n",
    "print(time.strftime('%a %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape=(5, 22)))\n",
    "model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['categorical_accuracy'])\n",
    "\n",
    "val-loss: 1.092895119583013"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape=(5, 22)))\n",
    "model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['categorical_accuracy'])\n",
    "history = model.fit(x_train1, y_train, validation_split=0.2, nb_epoch=130, batch_size=100, verbose = 5)\n",
    "\n",
    "val-loss: 1.0902978119526525"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape=(5, 22), dropout_W=0.0, dropout_U=0.2))\n",
    "model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['categorical_accuracy'])\n",
    "history = model.fit(x_train1, y_train, validation_split=0.2, nb_epoch=170, batch_size=100, verbose = 10)\n",
    "\n",
    "val-loss: 1.0739943640246441"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm + dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "model1 = Sequential()\n",
    "model1.add(LSTM(150, input_shape=(5, 22), dropout_W=0.0, dropout_U=0.3))\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(150, input_dim=len(col2), init='uniform', activation='relu'))\n",
    "model2.add(Dropout(0.3))\n",
    "\n",
    "merged = Merge([model1,model2], mode='concat')\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "final_model.add(Dense(64, init='uniform', activation='relu'))\n",
    "final_model.add(Dropout(0.3))\n",
    "final_model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "final_model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['categorical_accuracy'])\n",
    "\n",
    "print(final_model.summary())\n",
    "final_model.fit([x_train1, x_train2], y_train, validation_split=0.2, nb_epoch=170, batch_size=100, verbose = 2)\n",
    "\n",
    "print(time.strftime('%a %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model1 = Sequential()\n",
    "model1.add(LSTM(150, input_shape=(5, 22)))\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(150, input_dim=len(col2), init='uniform', activation='relu'))\n",
    "merged = Merge([model1,model2], mode='concat')\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "final_model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "final_model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['categorical_accuracy'])\n",
    "final_model.fit([x_train1, x_train2], y_train, validation_split=0.2, nb_epoch=130, batch_size=100, verbose = 10)\n",
    "\n",
    "val-loss: 0.99613085681124991"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model1 = Sequential()\n",
    "model1.add(LSTM(150, input_shape=(5, 22), dropout_U=0.2))\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(150, input_dim=len(col2), init='uniform', activation='relu'))\n",
    "merged = Merge([model1,model2], mode='concat')\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "final_model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "final_model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['categorical_accuracy'])\n",
    "final_model.fit([x_train1, x_train2], y_train, validation_split=0.2, nb_epoch=170, batch_size=100, verbose = 10)\n",
    "\n",
    "val-loss: 1.0739943640246441"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model1 = Sequential()\n",
    "model1.add(LSTM(150, input_shape=(5, 22), dropout_W=0.0, dropout_U=0.2))\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(150, input_dim=len(col2), init='uniform', activation='relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "merged = Merge([model1,model2], mode='concat')\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "final_model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "final_model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['categorical_accuracy'])\n",
    "final_model.fit([x_train1, x_train2], y_train, validation_split=0.2, nb_epoch=130, batch_size=100, verbose = 2)\n",
    "\n",
    "val-loss: 1.0077"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model1 = Sequential()\n",
    "model1.add(LSTM(150, input_shape=(5, 22), dropout_W=0.0, dropout_U=0.3))\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(150, input_dim=len(col2), init='uniform', activation='relu'))\n",
    "model2.add(Dropout(0.3))\n",
    "merged = Merge([model1,model2], mode='concat')\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "final_model.add(Dense(64, init='uniform', activation='relu'))\n",
    "final_model.add(Dropout(0.3))\n",
    "final_model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "final_model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['categorical_accuracy'])\n",
    "final_model.fit([x_train1, x_train2], y_train, validation_split=0.2, nb_epoch=170, batch_size=100, verbose = 2)\n",
    "\n",
    "val-loss: 1.0238"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution1D: time_wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution1D(60, 22, border_mode='same', input_shape=(5, 22),activation = 'relu'))\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(64, init='uniform', activation='relu'))\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "history = model.fit(x_train1, y_train, validation_split=0.2, nb_epoch=25, batch_size=100, verbose = 2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution1D(60, 22, border_mode='same', input_shape=(5, 22),activation = 'relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "history = model.fit(x_train1, y_train, validation_split=0.2, nb_epoch=25, batch_size=100, verbose = 2)\n",
    "\n",
    "val-loss: 1.0572"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution1d: product-wise and  time-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Convolution1D(60, 22, border_mode='same', input_shape=(5, 22),activation = 'relu'))\n",
    "model1.add(Flatten())\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Convolution1D(30, 5, border_mode='same', input_shape=(22, 5),activation = 'relu'))\n",
    "model2.add(Flatten())\n",
    "\n",
    "merged = Merge([model1,model2], mode='concat')\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "final_model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "final_model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['categorical_accuracy'])\n",
    "print(final_model.summary())\n",
    "\n",
    "history = final_model.fit([x_train1, x_train1.transpose((0, 2, 1))], y_train, validation_split=0.2, nb_epoch=30, batch_size=100, verbose = 2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Convolution1D(60, 22, border_mode='same', input_shape=(5, 22),activation = 'relu'))\n",
    "model1.add(Flatten())\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Convolution1D(30, 5, border_mode='same', input_shape=(22, 5),activation = 'relu'))\n",
    "model2.add(Flatten())\n",
    "\n",
    "merged = Merge([model1,model2], mode='concat')\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "final_model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "final_model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['categorical_accuracy'])\n",
    "history = final_model.fit([x_train1, x_train1.transpose((0, 2, 1))], y_train, validation_split=0.2, nb_epoch=30, batch_size=100, verbose = 2)\n",
    "\n",
    "val_loss: 1.0281"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution1d: demographic-wise + product-wise + time+wise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_product = product_col_1 + product_col_2 + product_col_3 + product_col_4 + product_col_5\n",
    "col_quant = ['age','antiguedad','renta','log_age','square_age','log_renta','square_renta','log_antiguedad','square_antiguedad']\n",
    "col_past = [col for col in df_train.columns if '_past' in col]\n",
    "col_demo = list(df_train.drop(['target','ncodpers']+col_product+col_quant+col_past, 1).columns.values)\n",
    "\n",
    "x_train1 = df_train[col_product].as_matrix()\n",
    "x_test1 = df_test[col_product].as_matrix()\n",
    "\n",
    "x_train1 = np.reshape(x_train1, (len(x_train1), 5, 22))\n",
    "x_test1 = np.reshape(x_test1, (len(x_test1), 5, 22))\n",
    "\n",
    "x_train_quant = df_train[col_quant].as_matrix()\n",
    "x_train_quant = np.reshape(x_train_quant, (len(x_train_quant), 1, len(col_quant)))\n",
    "x_test_quant = df_test[col_quant].as_matrix()\n",
    "x_test_quant = np.reshape(x_test_quant, (len(x_test_quant), 1, len(col_quant)))\n",
    "\n",
    "x_train_past = df_train[col_past].as_matrix()\n",
    "x_train_past = np.reshape(x_train_past, (len(x_train_past), 1, len(col_past)))\n",
    "x_test_past = df_test[col_past].as_matrix()\n",
    "x_test_past = np.reshape(x_test_past, (len(x_test_past), 1, len(col_past)))\n",
    "\n",
    "x_train_demo = df_train[col_demo].as_matrix()\n",
    "x_train_demo = np.reshape(x_train_demo, (len(x_train_demo), 1, len(col_demo)))\n",
    "x_test_demo = df_test[col_demo].as_matrix()\n",
    "x_test_demo = np.reshape(x_test_demo, (len(x_test_demo), 1, len(col_demo)))\n",
    "\n",
    "y_train = pd.get_dummies(df_train['target'].astype(int)).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### product-wise\n",
    "model1 = Sequential()\n",
    "model1.add(Convolution1D(60, 22, border_mode='same', input_shape=(5, 22),activation = 'relu'))\n",
    "model1.add(Flatten())\n",
    "\n",
    "# time-wise\n",
    "model2 = Sequential()\n",
    "model2.add(Convolution1D(30, 5, border_mode='same', input_shape=(22, 5),activation = 'relu'))\n",
    "model2.add(Flatten())\n",
    "\n",
    "# quantitative-wise\n",
    "model3 = Sequential()\n",
    "model3.add(Convolution1D(30, len(col_quant), border_mode='same',input_shape=(1, len(col_quant)),activation = 'relu'))\n",
    "model3.add(Flatten())\n",
    "\n",
    "# past_product-wise\n",
    "model4 = Sequential()\n",
    "model4.add(Convolution1D(30, len(col_past), border_mode='same',input_shape=(1, len(col_past)),activation = 'relu'))\n",
    "model4.add(Flatten())\n",
    "\n",
    "# demographic-wise\n",
    "model5 = Sequential()\n",
    "model5.add(Convolution1D(30, len(col_demo), border_mode='same',input_shape=(1, len(col_demo)),activation = 'relu'))\n",
    "model5.add(Flatten())\n",
    "#model3.add(Dense(150, input_dim=len(col2), init='uniform', activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "merged = Merge([model1,model2,model3,model4,model5], mode='concat')\n",
    "\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "final_model.add(Dense(150, init='uniform', activation='relu'))\n",
    "final_model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "final_model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['categorical_accuracy'])\n",
    "print(final_model.summary())\n",
    "\n",
    "history = final_model.fit([x_train1, x_train1.transpose((0, 2, 1)), x_train_quant, x_train_past, x_train_demo], y_train, validation_split=0.2, nb_epoch=100, batch_size=100, verbose = 2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### product-wise\n",
    "model1 = Sequential()\n",
    "model1.add(Convolution1D(60, 22, border_mode='same', input_shape=(5, 22),activation = 'relu'))\n",
    "model1.add(Flatten())\n",
    "\n",
    "# time-wise\n",
    "model2 = Sequential()\n",
    "model2.add(Convolution1D(30, 5, border_mode='same', input_shape=(22, 5),activation = 'relu'))\n",
    "model2.add(Flatten())\n",
    "\n",
    "# domegraphic-wise\n",
    "model3 = Sequential()\n",
    "model3.add(Convolution1D(300, len(col2), border_mode='same',input_shape=(1, len(col2)),activation = 'relu'))\n",
    "model3.add(Flatten())\n",
    "#model3.add(Dense(150, input_dim=len(col2), init='uniform', activation='relu'))\n",
    "\n",
    "merged = Merge([model1,model2,model3], mode='concat')\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "final_model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "final_model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['categorical_accuracy'])\n",
    "print(final_model.summary())\n",
    "\n",
    "history = final_model.fit([x_train1, x_train1.transpose((0, 2, 1)), x_train3], y_train, validation_split=0.2, nb_epoch=100, batch_size=100, verbose = 2)\n",
    "\n",
    "val_loss: 0.9875"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution1d: time-wise + product-wise + demographic dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### product-wise\n",
    "model1 = Sequential()\n",
    "model1.add(Convolution1D(60, 22, border_mode='same', input_shape=(5, 22),activation = 'relu'))\n",
    "model1.add(Flatten())\n",
    "\n",
    "# time-wise\n",
    "model2 = Sequential()\n",
    "model2.add(Convolution1D(30, 5, border_mode='same', input_shape=(22, 5),activation = 'relu'))\n",
    "model2.add(Flatten())\n",
    "\n",
    "# domegraphic-wise\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(150, input_dim=len(col2), init='uniform', activation='relu'))\n",
    "\n",
    "merged = Merge([model1,model2,model3], mode='concat')\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "final_model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "final_model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['categorical_accuracy'])\n",
    "print(final_model.summary())\n",
    "\n",
    "history = final_model.fit([x_train1, x_train1.transpose((0, 2, 1)), x_train2], y_train,validation_split=0.2, nb_epoch=100, batch_size=100, verbose = 2)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### product-wise\n",
    "model1 = Sequential()\n",
    "model1.add(Convolution1D(60, 22, border_mode='same', input_shape=(5, 22),activation = 'relu'))\n",
    "model1.add(Flatten())\n",
    "\n",
    "# time-wise\n",
    "model2 = Sequential()\n",
    "model2.add(Convolution1D(30, 5, border_mode='same', input_shape=(22, 5),activation = 'relu'))\n",
    "model2.add(Flatten())\n",
    "\n",
    "# domegraphic-wise\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(150, input_dim=len(col2), init='uniform', activation='relu'))\n",
    "\n",
    "merged = Merge([model1,model2,model3], mode='concat')\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "final_model.add(Dense(22, init='uniform', activation='softmax'))\n",
    "final_model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['categorical_accuracy'])\n",
    "print(final_model.summary())\n",
    "\n",
    "history = final_model.fit([x_train1, x_train1.transpose((0, 2, 1)), x_train2], y_train,validation_split=0.2, nb_epoch=100, batch_size=100, verbose = 2)\n",
    "\n",
    "\n",
    "val_loss: 0.9861"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = list(df_train.drop(['target','ncodpers'], 1).columns.values)\n",
    "\n",
    "id_preds = defaultdict(list)\n",
    "ids = df_test['ncodpers'].values\n",
    " \n",
    "# predict model \n",
    "y_train = df_train['target']\n",
    "x_train = df_train[cols]\n",
    "\n",
    "print(\"Validating...\")\n",
    "\n",
    "param = {}\n",
    "param['objective'] = 'multi:softprob'\n",
    "param['seed'] = 0\n",
    "param['silent'] = 0\n",
    "param['eval_metric'] = \"mlogloss\"\n",
    "param['booster'] = 'gbtree'\n",
    "param['eta'] = 0.1\n",
    "param['num_class'] = 22\n",
    "param['colsample_bytree'] = 0.9\n",
    "param['subsample'] = 0.9\n",
    "param['max_depth'] = 6\n",
    "param['min_child_weight'] = 2\n",
    "param['reg_lambda'] =100\n",
    "num_round = 170\n",
    "\n",
    "plst = list(param.items())\n",
    "  \n",
    "xgtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "\n",
    "xgb.cv(param, xgtrain, num_round, nfold=3,\n",
    "       metrics={'mlogloss'}, seed = 0,\n",
    "       callbacks=[xgb.callback.print_evaluation(show_stdv=True)])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
