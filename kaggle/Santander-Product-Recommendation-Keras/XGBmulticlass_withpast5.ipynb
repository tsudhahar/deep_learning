{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demographic_cols = ['ncodpers','fecha_alta','ind_empleado','pais_residencia','sexo','age','ind_nuevo','antiguedad','indrel',\n",
    " 'indrel_1mes','tiprel_1mes','indresi','indext','conyuemp','canal_entrada','indfall',\n",
    " 'tipodom','cod_prov','ind_actividad_cliente','renta','segmento']\n",
    "\n",
    "notuse = [\"ult_fec_cli_1t\",\"nomprov\",'fecha_dato']\n",
    "\n",
    "product_col = [\n",
    " 'ind_ahor_fin_ult1','ind_aval_fin_ult1','ind_cco_fin_ult1','ind_cder_fin_ult1','ind_cno_fin_ult1','ind_ctju_fin_ult1',\n",
    " 'ind_ctma_fin_ult1','ind_ctop_fin_ult1','ind_ctpp_fin_ult1','ind_deco_fin_ult1','ind_deme_fin_ult1',\n",
    " 'ind_dela_fin_ult1','ind_ecue_fin_ult1','ind_fond_fin_ult1','ind_hip_fin_ult1','ind_plan_fin_ult1',\n",
    " 'ind_pres_fin_ult1','ind_reca_fin_ult1','ind_tjcr_fin_ult1','ind_valo_fin_ult1','ind_viv_fin_ult1','ind_nomina_ult1',\n",
    " 'ind_nom_pens_ult1','ind_recibo_ult1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/r/.local/lib/python3.4/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (4,7,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('cleaned_data/DataMulticlass_6_withpast2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/r/.local/lib/python3.4/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('cleaned_data/TestSet_withpast3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_train['fecha_alta'] = df_train['fecha_alta'].apply(lambda x: x.toordinal()-(1990*365))\n",
    "df_test['fecha_alta'] = df_test['fecha_alta'].apply(lambda x: x.toordinal()-(1990*365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_data(df):\n",
    "    df = df[df['ind_nuevo'] == 0]\n",
    "    df = df[df['antiguedad'] != -999999]\n",
    "    df = df[df['indrel'] == 1]\n",
    "    df = df[df['indresi'] == 'S']\n",
    "    df = df[df['indfall'] == 'N']\n",
    "    df = df[df['tipodom'] == 1]\n",
    "    df = df[df['ind_empleado'] == 'N']\n",
    "    df = df[df['pais_residencia'] == 'ES']\n",
    "    df = df[df['indrel_1mes'] == 1]\n",
    "    df = df[df['tiprel_1mes'] == ('A' or 'I')]\n",
    "    df = df[df['indext'] == 'N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter_data(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unneccessary column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_column = ['ind_nuevo','indrel','indresi','indfall','tipodom','ind_empleado','pais_residencia','indrel_1mes','indext','conyuemp','fecha_alta','tiprel_1mes']\n",
    "\n",
    "df_train.drop(drop_column, axis=1, inplace = True)\n",
    "df_test.drop(drop_column, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add missing income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test[\"renta\"]   = pd.to_numeric(df_test[\"renta\"], errors=\"coerce\")\n",
    "unique_prov = df_test[df_test.cod_prov.notnull()].cod_prov.unique()\n",
    "grouped = df_test.groupby(\"cod_prov\")[\"renta\"].median()\n",
    "\n",
    "def impute_renta(df):\n",
    "    df[\"renta\"]   = pd.to_numeric(df[\"renta\"], errors=\"coerce\")       \n",
    "    for cod in unique_prov:\n",
    "        df.loc[df['cod_prov']==cod,['renta']] = df.loc[df['cod_prov']==cod,['renta']].fillna({'renta':grouped[cod]}).values\n",
    "    df.renta.fillna(df_test[\"renta\"].median(), inplace=True)\n",
    "    \n",
    "impute_renta(df_train)\n",
    "impute_renta(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def drop_na(df):\n",
    "    df.dropna(axis = 0, subset = ['ind_actividad_cliente'], inplace = True)\n",
    "    \n",
    "drop_na(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert and make dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These column are categories feature, I'll transform them using get_dummy\n",
    "dummy_col = ['sexo','canal_entrada','cod_prov','segmento']\n",
    "dummy_col_select = ['canal_entrada','cod_prov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "limit = int(0.05 * len(df_train.index))\n",
    "use_dummy_col = {}\n",
    "\n",
    "for col in dummy_col_select:\n",
    "    trainlist = df_train[col].value_counts()\n",
    "    use_dummy_col[col] = []\n",
    "    for i,item in enumerate(trainlist):\n",
    "        if item > limit:\n",
    "            use_dummy_col[col].append(df_train[col].value_counts().index[i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dummy(df):\n",
    "    for col in dummy_col_select:\n",
    "        for item in df[col].unique(): \n",
    "            if item not in use_dummy_col[col]:\n",
    "                row_index = df[col] == item\n",
    "                df.loc[row_index,col] = np.nan\n",
    "    return pd.get_dummies(df, prefix=dummy_col, columns = dummy_col)\n",
    "    \n",
    "df_train = get_dummy(df_train)\n",
    "df_test = get_dummy(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_age(df):\n",
    "    df[\"age\"]   = pd.to_numeric(df[\"age\"], errors=\"coerce\")\n",
    "    max_age = 80 \n",
    "    log_max_age = np.log(max_age) \n",
    "    square_max_age  = np.square(max_age)\n",
    "    df[\"age\"]   = df['age'].apply(lambda x: min(x ,max_age))\n",
    "    df[\"log_age\"]   = df['age'].apply(lambda x: round(np.log10(x+1)/log_max_age, 6))\n",
    "    df[\"square_age\"]   = df['age'].apply(lambda x: round(np.square(x)/square_max_age, 6))\n",
    "    df[\"age\"]   = df['age'].apply(lambda x: round( x/max_age, 6))\n",
    "\n",
    "def clean_renta(df):\n",
    "    max_renta = 1.0e6\n",
    "    log_max_renta = np.log(max_renta) \n",
    "    square_max_renta  = np.square(max_renta)\n",
    "    df[\"renta\"]   = df['renta'].apply(lambda x: min(x ,max_renta))\n",
    "    df[\"log_renta\"]   = df['renta'].apply(lambda x: round(np.log10(x+1)/log_max_renta, 6))\n",
    "    df[\"square_renta\"]   = df['renta'].apply(lambda x: round(np.square(x)/square_max_renta, 6))\n",
    "    df[\"renta\"]   = df['renta'].apply(lambda x: round( x/max_renta, 6))\n",
    "    \n",
    "def clean_antigue(df):\n",
    "    df[\"antiguedad\"]   = pd.to_numeric(df[\"antiguedad\"], errors=\"coerce\")\n",
    "    df[\"antiguedad\"] = df[\"antiguedad\"].replace(-999999, df['antiguedad'].median())\n",
    "    max_antigue = 256\n",
    "    log_max_antigue = np.log(max_antigue) \n",
    "    square_max_antigue  = np.square(max_antigue)\n",
    "    df[\"antiguedad\"]   = df['antiguedad'].apply(lambda x: min(x ,max_antigue))\n",
    "    df[\"log_antiguedad\"]   = df['antiguedad'].apply(lambda x: round(np.log10(x+1)/log_max_antigue, 6))\n",
    "    df[\"square_antiguedad\"]   = df['antiguedad'].apply(lambda x: round(np.square(x)/square_max_antigue, 6))\n",
    "    df[\"antiguedad\"]   = df['antiguedad'].apply(lambda x: round( x/max_antigue, 6))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_age(df_train)\n",
    "clean_age(df_test)\n",
    "\n",
    "clean_renta(df_train)\n",
    "clean_renta(df_test)\n",
    "\n",
    "clean_antigue(df_train)\n",
    "clean_antigue(df_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "product_col_5 = [col for col in df_train.columns if '_ult1_5' in col]\n",
    "product_col_4 = [col for col in df_train.columns if '_ult1_4' in col]\n",
    "product_col_3 = [col for col in df_train.columns if '_ult1_3' in col]\n",
    "product_col_2 = [col for col in df_train.columns if '_ult1_2' in col]\n",
    "product_col_1 = [col for col in df_train.columns if '_ult1_1' in col]\n",
    "\n",
    "df_train['tot'] = df_train[product_col_5].sum(axis=1)\n",
    "df_test['tot'] = df_test[product_col_5].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in product_col[2:]:\n",
    "    df_train[col+'_past'] = (df_train[col+'_5']+df_train[col+'_4']+df_train[col+'_3']+df_train[col+'_2']+df_train[col+'_1'])/5\n",
    "    df_test[col+'_past'] = (df_test[col+'_5']+df_test[col+'_4']+df_test[col+'_3']+df_test[col+'_2']+df_test[col+'_1'])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pro in product_col[2:]:\n",
    "    df_train[pro+'_past'] = df_train[pro+'_past']*(1-df_train[pro+'_5'])\n",
    "    df_test[pro+'_past'] = df_test[pro+'_past']*(1-df_test[pro+'_5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, colsample_bytree=0.9, max_depth= 6, eta=0.1, min_child_weight=2, subsample=0.9, num_rounds=150):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['seed'] = 0\n",
    "    param['silent'] = 0\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['booster'] = 'gbtree'\n",
    "    param['num_class'] = 22\n",
    "    param['reg_lambda'] =100\n",
    "    param['colsample_bytree'] = colsample_bytree\n",
    "    param['max_depth'] = max_depth \n",
    "    param['eta'] = eta\n",
    "    param['min_child_weight'] = min_child_weight\n",
    "    param['subsample'] = subsample\n",
    "    num_round = num_rounds\n",
    "\n",
    "    progress = dict()\n",
    "    plst = list(param.items())\n",
    "    \n",
    "    #xgtrain = xgb.DMatrix(train_X.loc[df_train.index.values%10!=0], label=train_y.loc[df_train.index.values%10!=0])\n",
    "    #xgtest = xgb.DMatrix(train_X.loc[df_train.index.values%10==0], label=train_y.loc[df_train.index.values%10==0])\n",
    "    #watchlist  = [(xgtrain,'train'),(xgtest,'test')]\n",
    "    #model = xgb.train(plst, xgtrain, num_rounds,watchlist,evals_result=progress)\n",
    "    \n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "    watchlist  = [(xgtrain,'train')]\n",
    "    model = xgb.train(plst, xgtrain, int(num_rounds/0.9), watchlist, evals_result=progress)\n",
    "    return (model, progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = list(df_train.drop(['target','ncodpers'], 1).columns.values)\n",
    "\n",
    "id_preds = defaultdict(list)\n",
    "ids = df_test['ncodpers'].values\n",
    "\n",
    "# predict model \n",
    "y_train = df_train['target']\n",
    "x_train = df_train[cols]\n",
    "    \n",
    "(clf, progress) = runXGB(x_train, y_train)\n",
    "          \n",
    "x_test = df_test[cols]\n",
    "x_test = x_test.fillna(0) # check this\n",
    "        \n",
    "d_test = xgb.DMatrix(x_test)\n",
    "p_test = clf.predict(d_test)\n",
    "        \n",
    "for id, p in zip(ids, p_test):\n",
    "    #id_preds[id] = list(p)\n",
    "    id_preds[id] = [0,0] + list(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(progress['test']['mlogloss'])\n",
    "plt.plot(progress['train']['mlogloss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Product Ranking "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "product_list = df_train[product_col_prev].sum(axis=0)/(df_train[product_col_prev].sum(axis=0).sum())\n",
    "\n",
    "id_preds2 = {}\n",
    "for row in df_test.values:\n",
    "    id = row[0]\n",
    "    id_preds2[id] = product_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fraction = 0.8\n",
    "id_preds_combined = {}\n",
    "\n",
    "for uid, p in id_preds.items():\n",
    "    id_preds_combined[uid] = fraction*np.asarray(id_preds[uid]) + (1-fraction)*np.asarray(id_preds3[uid])\n",
    "    \n",
    "id_preds = id_preds_combined    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_recent =  pd.read_csv('cleaned_data/df_recent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv('input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check if customer already have each product or not. \n",
    "already_active = {}\n",
    "for row in df_recent.values:\n",
    "    row = list(row)\n",
    "    id = row.pop(0)\n",
    "    active = [c[0] for c in zip(tuple(product_col), row) if c[1] > 0]\n",
    "    already_active[id] = active\n",
    "\n",
    "# add 7 products(that user don't have yet), higher probability first -> train_pred   \n",
    "train_preds = {}\n",
    "for id, p in id_preds.items():\n",
    "    preds = [i[0] for i in sorted([i for i in zip(tuple(product_col), p) if i[0] not in already_active[id]],\n",
    "                                  key=lambda i:i [1], \n",
    "                                  reverse=True)[:7]]\n",
    "    train_preds[id] = preds\n",
    "    \n",
    "test_preds = []\n",
    "for row in sample.values:\n",
    "    id = row[0]\n",
    "    p = train_preds[id]\n",
    "    test_preds.append(' '.join(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample['added_products'] = test_preds\n",
    "sample.to_csv('output/XGBmulticlass_withpast5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Validation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n",
      "[0]\ttrain-mlogloss:2.72088+0.00153514\ttest-mlogloss:2.7226+0.000930488\n",
      "[1]\ttrain-mlogloss:2.4869+0.00621075\ttest-mlogloss:2.48924+0.0100276\n",
      "[2]\ttrain-mlogloss:2.30551+0.00410903\ttest-mlogloss:2.30883+0.00869148\n",
      "[3]\ttrain-mlogloss:2.16584+0.0029511\ttest-mlogloss:2.16984+0.00812009\n",
      "[4]\ttrain-mlogloss:2.04793+0.00155158\ttest-mlogloss:2.05291+0.00714633\n",
      "[5]\ttrain-mlogloss:1.94851+0.00157262\ttest-mlogloss:1.95431+0.00726233\n",
      "[6]\ttrain-mlogloss:1.86207+0.00152311\ttest-mlogloss:1.86866+0.00674262\n",
      "[7]\ttrain-mlogloss:1.78633+0.00137277\ttest-mlogloss:1.79355+0.00669338\n",
      "[8]\ttrain-mlogloss:1.71954+0.00139916\ttest-mlogloss:1.72742+0.00621573\n",
      "[9]\ttrain-mlogloss:1.6604+0.00150112\ttest-mlogloss:1.66905+0.0061865\n",
      "[10]\ttrain-mlogloss:1.60793+0.00171367\ttest-mlogloss:1.61722+0.00598067\n",
      "[11]\ttrain-mlogloss:1.56051+0.00183211\ttest-mlogloss:1.57038+0.00615759\n",
      "[12]\ttrain-mlogloss:1.51787+0.00195822\ttest-mlogloss:1.52839+0.00625863\n",
      "[13]\ttrain-mlogloss:1.47907+0.00209964\ttest-mlogloss:1.49024+0.00630704\n",
      "[14]\ttrain-mlogloss:1.44385+0.00235094\ttest-mlogloss:1.45574+0.00617893\n",
      "[15]\ttrain-mlogloss:1.41184+0.00247743\ttest-mlogloss:1.42441+0.00631544\n",
      "[16]\ttrain-mlogloss:1.38243+0.00252242\ttest-mlogloss:1.39576+0.00642553\n",
      "[17]\ttrain-mlogloss:1.35551+0.00249257\ttest-mlogloss:1.36947+0.00657374\n",
      "[18]\ttrain-mlogloss:1.33078+0.00251102\ttest-mlogloss:1.34543+0.00665163\n",
      "[19]\ttrain-mlogloss:1.3077+0.0025639\ttest-mlogloss:1.32297+0.00674123\n",
      "[20]\ttrain-mlogloss:1.28668+0.00281723\ttest-mlogloss:1.30261+0.00655963\n",
      "[21]\ttrain-mlogloss:1.26718+0.00276988\ttest-mlogloss:1.28374+0.00675955\n",
      "[22]\ttrain-mlogloss:1.24923+0.00288402\ttest-mlogloss:1.26636+0.00673068\n",
      "[23]\ttrain-mlogloss:1.23274+0.00296431\ttest-mlogloss:1.25046+0.00674505\n",
      "[24]\ttrain-mlogloss:1.21726+0.00301035\ttest-mlogloss:1.23554+0.00683316\n",
      "[25]\ttrain-mlogloss:1.20291+0.00299956\ttest-mlogloss:1.22182+0.00688832\n",
      "[26]\ttrain-mlogloss:1.1895+0.00310488\ttest-mlogloss:1.209+0.00695907\n",
      "[27]\ttrain-mlogloss:1.17713+0.00324095\ttest-mlogloss:1.1972+0.00684079\n",
      "[28]\ttrain-mlogloss:1.16547+0.00325111\ttest-mlogloss:1.18613+0.00681863\n",
      "[29]\ttrain-mlogloss:1.15462+0.00328954\ttest-mlogloss:1.17579+0.00676514\n",
      "[30]\ttrain-mlogloss:1.14429+0.00333119\ttest-mlogloss:1.16607+0.00677619\n",
      "[31]\ttrain-mlogloss:1.13466+0.00325517\ttest-mlogloss:1.15707+0.00685613\n",
      "[32]\ttrain-mlogloss:1.12562+0.00334032\ttest-mlogloss:1.14863+0.00681578\n",
      "[33]\ttrain-mlogloss:1.11713+0.00331618\ttest-mlogloss:1.14072+0.00679313\n",
      "[34]\ttrain-mlogloss:1.10914+0.003294\ttest-mlogloss:1.13335+0.00683937\n",
      "[35]\ttrain-mlogloss:1.10167+0.00339598\ttest-mlogloss:1.12636+0.00676793\n",
      "[36]\ttrain-mlogloss:1.09447+0.00341692\ttest-mlogloss:1.11977+0.00675412\n",
      "[37]\ttrain-mlogloss:1.08784+0.00342898\ttest-mlogloss:1.1137+0.00679201\n",
      "[38]\ttrain-mlogloss:1.08149+0.00337852\ttest-mlogloss:1.10792+0.00689024\n",
      "[39]\ttrain-mlogloss:1.07551+0.00335675\ttest-mlogloss:1.10252+0.0069494\n",
      "[40]\ttrain-mlogloss:1.06977+0.00340524\ttest-mlogloss:1.09735+0.0068879\n",
      "[41]\ttrain-mlogloss:1.06436+0.00343607\ttest-mlogloss:1.09251+0.00687933\n",
      "[42]\ttrain-mlogloss:1.0593+0.00345725\ttest-mlogloss:1.08803+0.00686421\n",
      "[43]\ttrain-mlogloss:1.05443+0.00342576\ttest-mlogloss:1.08368+0.00692162\n",
      "[44]\ttrain-mlogloss:1.04984+0.0034322\ttest-mlogloss:1.07964+0.00691454\n",
      "[45]\ttrain-mlogloss:1.04535+0.00332569\ttest-mlogloss:1.07566+0.00698469\n",
      "[46]\ttrain-mlogloss:1.04107+0.00335553\ttest-mlogloss:1.07199+0.00695203\n",
      "[47]\ttrain-mlogloss:1.03697+0.00337106\ttest-mlogloss:1.0684+0.00692252\n",
      "[48]\ttrain-mlogloss:1.03306+0.00340854\ttest-mlogloss:1.06506+0.00686201\n",
      "[49]\ttrain-mlogloss:1.02937+0.00342988\ttest-mlogloss:1.06192+0.00680906\n",
      "[50]\ttrain-mlogloss:1.02577+0.00349569\ttest-mlogloss:1.05891+0.00680319\n",
      "[51]\ttrain-mlogloss:1.02241+0.00344405\ttest-mlogloss:1.05613+0.00680273\n",
      "[52]\ttrain-mlogloss:1.01915+0.00345308\ttest-mlogloss:1.05343+0.00678563\n",
      "[53]\ttrain-mlogloss:1.01599+0.00341136\ttest-mlogloss:1.05084+0.00679042\n",
      "[54]\ttrain-mlogloss:1.01297+0.00344919\ttest-mlogloss:1.04836+0.0068044\n",
      "[55]\ttrain-mlogloss:1.01+0.00338954\ttest-mlogloss:1.04596+0.00679602\n",
      "[56]\ttrain-mlogloss:1.00716+0.003363\ttest-mlogloss:1.04368+0.00678139\n",
      "[57]\ttrain-mlogloss:1.00446+0.00332541\ttest-mlogloss:1.04153+0.00682116\n",
      "[58]\ttrain-mlogloss:1.0019+0.00328038\ttest-mlogloss:1.03951+0.00683477\n",
      "[59]\ttrain-mlogloss:0.999413+0.00330942\ttest-mlogloss:1.03758+0.00686686\n",
      "[60]\ttrain-mlogloss:0.996996+0.00332564\ttest-mlogloss:1.03575+0.00689426\n",
      "[61]\ttrain-mlogloss:0.994691+0.00329306\ttest-mlogloss:1.03395+0.00691716\n",
      "[62]\ttrain-mlogloss:0.992361+0.00326997\ttest-mlogloss:1.03222+0.00690141\n",
      "[63]\ttrain-mlogloss:0.99018+0.00326425\ttest-mlogloss:1.0306+0.00689164\n",
      "[64]\ttrain-mlogloss:0.988086+0.00325599\ttest-mlogloss:1.02901+0.00688656\n",
      "[65]\ttrain-mlogloss:0.986001+0.00321097\ttest-mlogloss:1.02748+0.00697558\n",
      "[66]\ttrain-mlogloss:0.983958+0.0032501\ttest-mlogloss:1.02602+0.00695938\n",
      "[67]\ttrain-mlogloss:0.981993+0.00318442\ttest-mlogloss:1.02465+0.006983\n",
      "[68]\ttrain-mlogloss:0.980113+0.00323497\ttest-mlogloss:1.02336+0.00698887\n",
      "[69]\ttrain-mlogloss:0.978296+0.00325487\ttest-mlogloss:1.02206+0.00696679\n",
      "[70]\ttrain-mlogloss:0.976474+0.00320497\ttest-mlogloss:1.02083+0.00699054\n",
      "[71]\ttrain-mlogloss:0.97473+0.00317725\ttest-mlogloss:1.01968+0.00694012\n",
      "[72]\ttrain-mlogloss:0.973088+0.00318702\ttest-mlogloss:1.01859+0.00692741\n",
      "[73]\ttrain-mlogloss:0.971436+0.00318374\ttest-mlogloss:1.01754+0.00690309\n",
      "[74]\ttrain-mlogloss:0.969932+0.00319913\ttest-mlogloss:1.01647+0.00686504\n",
      "[75]\ttrain-mlogloss:0.968395+0.00324632\ttest-mlogloss:1.01547+0.00689976\n",
      "[76]\ttrain-mlogloss:0.966846+0.00322337\ttest-mlogloss:1.01451+0.0069038\n",
      "[77]\ttrain-mlogloss:0.965363+0.00322528\ttest-mlogloss:1.01354+0.0069279\n",
      "[78]\ttrain-mlogloss:0.963944+0.00323024\ttest-mlogloss:1.01269+0.00690613\n",
      "[79]\ttrain-mlogloss:0.962473+0.00319306\ttest-mlogloss:1.01183+0.00687896\n",
      "[80]\ttrain-mlogloss:0.961063+0.00314963\ttest-mlogloss:1.01104+0.00690406\n",
      "[81]\ttrain-mlogloss:0.95972+0.0031905\ttest-mlogloss:1.01022+0.00684758\n",
      "[82]\ttrain-mlogloss:0.958372+0.0032285\ttest-mlogloss:1.00946+0.00679099\n",
      "[83]\ttrain-mlogloss:0.95711+0.00323658\ttest-mlogloss:1.00871+0.00674809\n",
      "[84]\ttrain-mlogloss:0.955791+0.00317838\ttest-mlogloss:1.00792+0.00677326\n",
      "[85]\ttrain-mlogloss:0.954489+0.0031556\ttest-mlogloss:1.00714+0.00679939\n",
      "[86]\ttrain-mlogloss:0.953257+0.00321224\ttest-mlogloss:1.00641+0.00680148\n",
      "[87]\ttrain-mlogloss:0.952042+0.00321707\ttest-mlogloss:1.00585+0.00677494\n",
      "[88]\ttrain-mlogloss:0.950815+0.00318377\ttest-mlogloss:1.0052+0.00677135\n",
      "[89]\ttrain-mlogloss:0.949654+0.00317016\ttest-mlogloss:1.00461+0.00676032\n",
      "[90]\ttrain-mlogloss:0.948569+0.00315397\ttest-mlogloss:1.00403+0.00675411\n",
      "[91]\ttrain-mlogloss:0.947452+0.00313212\ttest-mlogloss:1.00347+0.00671248\n",
      "[92]\ttrain-mlogloss:0.94633+0.00309834\ttest-mlogloss:1.00293+0.00671216\n",
      "[93]\ttrain-mlogloss:0.945185+0.00311949\ttest-mlogloss:1.0024+0.00672718\n",
      "[94]\ttrain-mlogloss:0.944149+0.00306961\ttest-mlogloss:1.00187+0.00673192\n",
      "[95]\ttrain-mlogloss:0.943075+0.00304649\ttest-mlogloss:1.00137+0.00678334\n",
      "[96]\ttrain-mlogloss:0.942003+0.0030623\ttest-mlogloss:1.00091+0.00674126\n",
      "[97]\ttrain-mlogloss:0.940972+0.00302437\ttest-mlogloss:1.00045+0.00675698\n",
      "[98]\ttrain-mlogloss:0.939988+0.00298003\ttest-mlogloss:0.999984+0.00674681\n",
      "[99]\ttrain-mlogloss:0.939001+0.00297453\ttest-mlogloss:0.999544+0.00673569\n",
      "[100]\ttrain-mlogloss:0.938032+0.00297357\ttest-mlogloss:0.999136+0.00677281\n",
      "[101]\ttrain-mlogloss:0.937021+0.00291823\ttest-mlogloss:0.998695+0.0067507\n",
      "[102]\ttrain-mlogloss:0.936101+0.00296475\ttest-mlogloss:0.998231+0.00671563\n",
      "[103]\ttrain-mlogloss:0.935105+0.00299341\ttest-mlogloss:0.997857+0.00676105\n",
      "[104]\ttrain-mlogloss:0.934144+0.00298965\ttest-mlogloss:0.997512+0.00675388\n",
      "[105]\ttrain-mlogloss:0.933243+0.00301651\ttest-mlogloss:0.997111+0.00677191\n",
      "[106]\ttrain-mlogloss:0.932351+0.00303249\ttest-mlogloss:0.996758+0.00674875\n",
      "[107]\ttrain-mlogloss:0.931437+0.0030495\ttest-mlogloss:0.996428+0.00673869\n",
      "[108]\ttrain-mlogloss:0.930522+0.00302261\ttest-mlogloss:0.996097+0.00673263\n",
      "[109]\ttrain-mlogloss:0.929686+0.00301763\ttest-mlogloss:0.995822+0.00668634\n",
      "[110]\ttrain-mlogloss:0.928873+0.00297055\ttest-mlogloss:0.995524+0.00668431\n",
      "[111]\ttrain-mlogloss:0.928052+0.00299335\ttest-mlogloss:0.995164+0.00667818\n",
      "[112]\ttrain-mlogloss:0.927217+0.00302085\ttest-mlogloss:0.994903+0.00671893\n",
      "[113]\ttrain-mlogloss:0.926427+0.0030451\ttest-mlogloss:0.994656+0.00670589\n",
      "[114]\ttrain-mlogloss:0.925644+0.00303882\ttest-mlogloss:0.994386+0.00668549\n",
      "[115]\ttrain-mlogloss:0.924803+0.0030586\ttest-mlogloss:0.99415+0.00669374\n",
      "[116]\ttrain-mlogloss:0.923931+0.0030027\ttest-mlogloss:0.993867+0.00668613\n",
      "[117]\ttrain-mlogloss:0.923135+0.0030087\ttest-mlogloss:0.993639+0.00669402\n",
      "[118]\ttrain-mlogloss:0.922367+0.00298544\ttest-mlogloss:0.993395+0.00665899\n",
      "[119]\ttrain-mlogloss:0.921611+0.00299969\ttest-mlogloss:0.9932+0.00668204\n",
      "[120]\ttrain-mlogloss:0.920882+0.00299\ttest-mlogloss:0.992971+0.00665766\n",
      "[121]\ttrain-mlogloss:0.920082+0.00298457\ttest-mlogloss:0.992727+0.00664861\n",
      "[122]\ttrain-mlogloss:0.919318+0.00294209\ttest-mlogloss:0.992493+0.00663293\n",
      "[123]\ttrain-mlogloss:0.918628+0.00295569\ttest-mlogloss:0.992302+0.00662397\n",
      "[124]\ttrain-mlogloss:0.91788+0.00291151\ttest-mlogloss:0.99217+0.00661193\n",
      "[125]\ttrain-mlogloss:0.917155+0.00288648\ttest-mlogloss:0.991997+0.00664005\n",
      "[126]\ttrain-mlogloss:0.916448+0.00285681\ttest-mlogloss:0.991804+0.00665277\n",
      "[127]\ttrain-mlogloss:0.915739+0.00282617\ttest-mlogloss:0.991619+0.00663088\n",
      "[128]\ttrain-mlogloss:0.915047+0.00281665\ttest-mlogloss:0.99138+0.0066123\n",
      "[129]\ttrain-mlogloss:0.914368+0.00280504\ttest-mlogloss:0.991221+0.00661177\n",
      "[130]\ttrain-mlogloss:0.913651+0.00280089\ttest-mlogloss:0.99107+0.00657484\n",
      "[131]\ttrain-mlogloss:0.912948+0.00280428\ttest-mlogloss:0.990931+0.00659434\n",
      "[132]\ttrain-mlogloss:0.912276+0.00280554\ttest-mlogloss:0.990757+0.00656769\n",
      "[133]\ttrain-mlogloss:0.911604+0.00282283\ttest-mlogloss:0.990605+0.00653394\n",
      "[134]\ttrain-mlogloss:0.910943+0.002798\ttest-mlogloss:0.990495+0.00651831\n",
      "[135]\ttrain-mlogloss:0.910308+0.0028047\ttest-mlogloss:0.990393+0.00654083\n",
      "[136]\ttrain-mlogloss:0.909656+0.00283857\ttest-mlogloss:0.990283+0.00654754\n",
      "[137]\ttrain-mlogloss:0.908984+0.00282573\ttest-mlogloss:0.990099+0.00652756\n",
      "[138]\ttrain-mlogloss:0.908293+0.00279502\ttest-mlogloss:0.989945+0.00649821\n",
      "[139]\ttrain-mlogloss:0.907623+0.00273966\ttest-mlogloss:0.989835+0.00650667\n",
      "[140]\ttrain-mlogloss:0.906943+0.00271265\ttest-mlogloss:0.98975+0.00652743\n",
      "[141]\ttrain-mlogloss:0.906316+0.00272756\ttest-mlogloss:0.989653+0.00659269\n",
      "[142]\ttrain-mlogloss:0.905682+0.00279284\ttest-mlogloss:0.989518+0.00656183\n",
      "[143]\ttrain-mlogloss:0.905033+0.00277131\ttest-mlogloss:0.989404+0.00651662\n",
      "[144]\ttrain-mlogloss:0.904376+0.00277063\ttest-mlogloss:0.989299+0.00650846\n",
      "[145]\ttrain-mlogloss:0.903766+0.00276668\ttest-mlogloss:0.989176+0.00652728\n",
      "[146]\ttrain-mlogloss:0.903138+0.00278476\ttest-mlogloss:0.989123+0.00650624\n",
      "[147]\ttrain-mlogloss:0.902525+0.00277062\ttest-mlogloss:0.989003+0.00647395\n",
      "[148]\ttrain-mlogloss:0.901928+0.00281465\ttest-mlogloss:0.988923+0.00642421\n",
      "[149]\ttrain-mlogloss:0.901347+0.0028016\ttest-mlogloss:0.988865+0.00640186\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-mlogloss-mean</th>\n",
       "      <th>test-mlogloss-std</th>\n",
       "      <th>train-mlogloss-mean</th>\n",
       "      <th>train-mlogloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.722604</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>2.720884</td>\n",
       "      <td>0.001535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.489237</td>\n",
       "      <td>0.010028</td>\n",
       "      <td>2.486899</td>\n",
       "      <td>0.006211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.308827</td>\n",
       "      <td>0.008691</td>\n",
       "      <td>2.305514</td>\n",
       "      <td>0.004109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.169842</td>\n",
       "      <td>0.008120</td>\n",
       "      <td>2.165842</td>\n",
       "      <td>0.002951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.052907</td>\n",
       "      <td>0.007146</td>\n",
       "      <td>2.047926</td>\n",
       "      <td>0.001552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.954306</td>\n",
       "      <td>0.007262</td>\n",
       "      <td>1.948510</td>\n",
       "      <td>0.001573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.868662</td>\n",
       "      <td>0.006743</td>\n",
       "      <td>1.862075</td>\n",
       "      <td>0.001523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.793553</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>1.786333</td>\n",
       "      <td>0.001373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.727420</td>\n",
       "      <td>0.006216</td>\n",
       "      <td>1.719537</td>\n",
       "      <td>0.001399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.669053</td>\n",
       "      <td>0.006186</td>\n",
       "      <td>1.660403</td>\n",
       "      <td>0.001501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.617224</td>\n",
       "      <td>0.005981</td>\n",
       "      <td>1.607929</td>\n",
       "      <td>0.001714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.570376</td>\n",
       "      <td>0.006158</td>\n",
       "      <td>1.560507</td>\n",
       "      <td>0.001832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.528387</td>\n",
       "      <td>0.006259</td>\n",
       "      <td>1.517871</td>\n",
       "      <td>0.001958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.490241</td>\n",
       "      <td>0.006307</td>\n",
       "      <td>1.479071</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.455736</td>\n",
       "      <td>0.006179</td>\n",
       "      <td>1.443848</td>\n",
       "      <td>0.002351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.424415</td>\n",
       "      <td>0.006315</td>\n",
       "      <td>1.411839</td>\n",
       "      <td>0.002477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.395760</td>\n",
       "      <td>0.006426</td>\n",
       "      <td>1.382430</td>\n",
       "      <td>0.002522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.369474</td>\n",
       "      <td>0.006574</td>\n",
       "      <td>1.355510</td>\n",
       "      <td>0.002493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.345426</td>\n",
       "      <td>0.006652</td>\n",
       "      <td>1.330784</td>\n",
       "      <td>0.002511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.322968</td>\n",
       "      <td>0.006741</td>\n",
       "      <td>1.307698</td>\n",
       "      <td>0.002564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.302606</td>\n",
       "      <td>0.006560</td>\n",
       "      <td>1.286676</td>\n",
       "      <td>0.002817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.283741</td>\n",
       "      <td>0.006760</td>\n",
       "      <td>1.267177</td>\n",
       "      <td>0.002770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.266359</td>\n",
       "      <td>0.006731</td>\n",
       "      <td>1.249229</td>\n",
       "      <td>0.002884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.250464</td>\n",
       "      <td>0.006745</td>\n",
       "      <td>1.232737</td>\n",
       "      <td>0.002964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.235537</td>\n",
       "      <td>0.006833</td>\n",
       "      <td>1.217261</td>\n",
       "      <td>0.003010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.221824</td>\n",
       "      <td>0.006888</td>\n",
       "      <td>1.202912</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.208997</td>\n",
       "      <td>0.006959</td>\n",
       "      <td>1.189497</td>\n",
       "      <td>0.003105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.197201</td>\n",
       "      <td>0.006841</td>\n",
       "      <td>1.177130</td>\n",
       "      <td>0.003241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.186135</td>\n",
       "      <td>0.006819</td>\n",
       "      <td>1.165472</td>\n",
       "      <td>0.003251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.175794</td>\n",
       "      <td>0.006765</td>\n",
       "      <td>1.154621</td>\n",
       "      <td>0.003290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.992971</td>\n",
       "      <td>0.006658</td>\n",
       "      <td>0.920882</td>\n",
       "      <td>0.002990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.992727</td>\n",
       "      <td>0.006649</td>\n",
       "      <td>0.920082</td>\n",
       "      <td>0.002985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.992493</td>\n",
       "      <td>0.006633</td>\n",
       "      <td>0.919318</td>\n",
       "      <td>0.002942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.992302</td>\n",
       "      <td>0.006624</td>\n",
       "      <td>0.918628</td>\n",
       "      <td>0.002956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.992170</td>\n",
       "      <td>0.006612</td>\n",
       "      <td>0.917880</td>\n",
       "      <td>0.002912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.991997</td>\n",
       "      <td>0.006640</td>\n",
       "      <td>0.917155</td>\n",
       "      <td>0.002886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.991804</td>\n",
       "      <td>0.006653</td>\n",
       "      <td>0.916448</td>\n",
       "      <td>0.002857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.991619</td>\n",
       "      <td>0.006631</td>\n",
       "      <td>0.915739</td>\n",
       "      <td>0.002826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.991380</td>\n",
       "      <td>0.006612</td>\n",
       "      <td>0.915047</td>\n",
       "      <td>0.002817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.991221</td>\n",
       "      <td>0.006612</td>\n",
       "      <td>0.914368</td>\n",
       "      <td>0.002805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.991070</td>\n",
       "      <td>0.006575</td>\n",
       "      <td>0.913651</td>\n",
       "      <td>0.002801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.990931</td>\n",
       "      <td>0.006594</td>\n",
       "      <td>0.912948</td>\n",
       "      <td>0.002804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.990757</td>\n",
       "      <td>0.006568</td>\n",
       "      <td>0.912276</td>\n",
       "      <td>0.002806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.990605</td>\n",
       "      <td>0.006534</td>\n",
       "      <td>0.911604</td>\n",
       "      <td>0.002823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.990495</td>\n",
       "      <td>0.006518</td>\n",
       "      <td>0.910943</td>\n",
       "      <td>0.002798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.990393</td>\n",
       "      <td>0.006541</td>\n",
       "      <td>0.910308</td>\n",
       "      <td>0.002805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.990283</td>\n",
       "      <td>0.006548</td>\n",
       "      <td>0.909656</td>\n",
       "      <td>0.002839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.990099</td>\n",
       "      <td>0.006528</td>\n",
       "      <td>0.908984</td>\n",
       "      <td>0.002826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.989945</td>\n",
       "      <td>0.006498</td>\n",
       "      <td>0.908293</td>\n",
       "      <td>0.002795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.989835</td>\n",
       "      <td>0.006507</td>\n",
       "      <td>0.907623</td>\n",
       "      <td>0.002740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.989750</td>\n",
       "      <td>0.006527</td>\n",
       "      <td>0.906943</td>\n",
       "      <td>0.002713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.989653</td>\n",
       "      <td>0.006593</td>\n",
       "      <td>0.906316</td>\n",
       "      <td>0.002728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.989518</td>\n",
       "      <td>0.006562</td>\n",
       "      <td>0.905682</td>\n",
       "      <td>0.002793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.989404</td>\n",
       "      <td>0.006517</td>\n",
       "      <td>0.905033</td>\n",
       "      <td>0.002771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.989299</td>\n",
       "      <td>0.006508</td>\n",
       "      <td>0.904376</td>\n",
       "      <td>0.002771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.989176</td>\n",
       "      <td>0.006527</td>\n",
       "      <td>0.903766</td>\n",
       "      <td>0.002767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.989123</td>\n",
       "      <td>0.006506</td>\n",
       "      <td>0.903138</td>\n",
       "      <td>0.002785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.989003</td>\n",
       "      <td>0.006474</td>\n",
       "      <td>0.902525</td>\n",
       "      <td>0.002771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.988923</td>\n",
       "      <td>0.006424</td>\n",
       "      <td>0.901928</td>\n",
       "      <td>0.002815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.988865</td>\n",
       "      <td>0.006402</td>\n",
       "      <td>0.901347</td>\n",
       "      <td>0.002802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     test-mlogloss-mean  test-mlogloss-std  train-mlogloss-mean  \\\n",
       "0              2.722604           0.000930             2.720884   \n",
       "1              2.489237           0.010028             2.486899   \n",
       "2              2.308827           0.008691             2.305514   \n",
       "3              2.169842           0.008120             2.165842   \n",
       "4              2.052907           0.007146             2.047926   \n",
       "5              1.954306           0.007262             1.948510   \n",
       "6              1.868662           0.006743             1.862075   \n",
       "7              1.793553           0.006693             1.786333   \n",
       "8              1.727420           0.006216             1.719537   \n",
       "9              1.669053           0.006186             1.660403   \n",
       "10             1.617224           0.005981             1.607929   \n",
       "11             1.570376           0.006158             1.560507   \n",
       "12             1.528387           0.006259             1.517871   \n",
       "13             1.490241           0.006307             1.479071   \n",
       "14             1.455736           0.006179             1.443848   \n",
       "15             1.424415           0.006315             1.411839   \n",
       "16             1.395760           0.006426             1.382430   \n",
       "17             1.369474           0.006574             1.355510   \n",
       "18             1.345426           0.006652             1.330784   \n",
       "19             1.322968           0.006741             1.307698   \n",
       "20             1.302606           0.006560             1.286676   \n",
       "21             1.283741           0.006760             1.267177   \n",
       "22             1.266359           0.006731             1.249229   \n",
       "23             1.250464           0.006745             1.232737   \n",
       "24             1.235537           0.006833             1.217261   \n",
       "25             1.221824           0.006888             1.202912   \n",
       "26             1.208997           0.006959             1.189497   \n",
       "27             1.197201           0.006841             1.177130   \n",
       "28             1.186135           0.006819             1.165472   \n",
       "29             1.175794           0.006765             1.154621   \n",
       "..                  ...                ...                  ...   \n",
       "120            0.992971           0.006658             0.920882   \n",
       "121            0.992727           0.006649             0.920082   \n",
       "122            0.992493           0.006633             0.919318   \n",
       "123            0.992302           0.006624             0.918628   \n",
       "124            0.992170           0.006612             0.917880   \n",
       "125            0.991997           0.006640             0.917155   \n",
       "126            0.991804           0.006653             0.916448   \n",
       "127            0.991619           0.006631             0.915739   \n",
       "128            0.991380           0.006612             0.915047   \n",
       "129            0.991221           0.006612             0.914368   \n",
       "130            0.991070           0.006575             0.913651   \n",
       "131            0.990931           0.006594             0.912948   \n",
       "132            0.990757           0.006568             0.912276   \n",
       "133            0.990605           0.006534             0.911604   \n",
       "134            0.990495           0.006518             0.910943   \n",
       "135            0.990393           0.006541             0.910308   \n",
       "136            0.990283           0.006548             0.909656   \n",
       "137            0.990099           0.006528             0.908984   \n",
       "138            0.989945           0.006498             0.908293   \n",
       "139            0.989835           0.006507             0.907623   \n",
       "140            0.989750           0.006527             0.906943   \n",
       "141            0.989653           0.006593             0.906316   \n",
       "142            0.989518           0.006562             0.905682   \n",
       "143            0.989404           0.006517             0.905033   \n",
       "144            0.989299           0.006508             0.904376   \n",
       "145            0.989176           0.006527             0.903766   \n",
       "146            0.989123           0.006506             0.903138   \n",
       "147            0.989003           0.006474             0.902525   \n",
       "148            0.988923           0.006424             0.901928   \n",
       "149            0.988865           0.006402             0.901347   \n",
       "\n",
       "     train-mlogloss-std  \n",
       "0              0.001535  \n",
       "1              0.006211  \n",
       "2              0.004109  \n",
       "3              0.002951  \n",
       "4              0.001552  \n",
       "5              0.001573  \n",
       "6              0.001523  \n",
       "7              0.001373  \n",
       "8              0.001399  \n",
       "9              0.001501  \n",
       "10             0.001714  \n",
       "11             0.001832  \n",
       "12             0.001958  \n",
       "13             0.002100  \n",
       "14             0.002351  \n",
       "15             0.002477  \n",
       "16             0.002522  \n",
       "17             0.002493  \n",
       "18             0.002511  \n",
       "19             0.002564  \n",
       "20             0.002817  \n",
       "21             0.002770  \n",
       "22             0.002884  \n",
       "23             0.002964  \n",
       "24             0.003010  \n",
       "25             0.003000  \n",
       "26             0.003105  \n",
       "27             0.003241  \n",
       "28             0.003251  \n",
       "29             0.003290  \n",
       "..                  ...  \n",
       "120            0.002990  \n",
       "121            0.002985  \n",
       "122            0.002942  \n",
       "123            0.002956  \n",
       "124            0.002912  \n",
       "125            0.002886  \n",
       "126            0.002857  \n",
       "127            0.002826  \n",
       "128            0.002817  \n",
       "129            0.002805  \n",
       "130            0.002801  \n",
       "131            0.002804  \n",
       "132            0.002806  \n",
       "133            0.002823  \n",
       "134            0.002798  \n",
       "135            0.002805  \n",
       "136            0.002839  \n",
       "137            0.002826  \n",
       "138            0.002795  \n",
       "139            0.002740  \n",
       "140            0.002713  \n",
       "141            0.002728  \n",
       "142            0.002793  \n",
       "143            0.002771  \n",
       "144            0.002771  \n",
       "145            0.002767  \n",
       "146            0.002785  \n",
       "147            0.002771  \n",
       "148            0.002815  \n",
       "149            0.002802  \n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = list(df_train.drop(['target','ncodpers'], 1).columns.values)\n",
    "\n",
    "id_preds = defaultdict(list)\n",
    "ids = df_test['ncodpers'].values\n",
    " \n",
    "# predict model \n",
    "y_train = df_train['target']\n",
    "x_train = df_train[cols]\n",
    "\n",
    "print(\"Validating...\")\n",
    "\n",
    "param = {}\n",
    "param['objective'] = 'multi:softprob'\n",
    "param['seed'] = 0\n",
    "param['silent'] = 0\n",
    "param['eval_metric'] = \"mlogloss\"\n",
    "param['booster'] = 'gbtree'\n",
    "param['eta'] = 0.1\n",
    "param['num_class'] = 22\n",
    "param['colsample_bytree'] = 0.9\n",
    "param['subsample'] = 0.9\n",
    "param['max_depth'] = 6\n",
    "param['min_child_weight'] = 2\n",
    "param['reg_lambda'] =100\n",
    "num_round = 150\n",
    "\n",
    "plst = list(param.items())\n",
    "  \n",
    "xgtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "\n",
    "xgb.cv(param, xgtrain, num_round, nfold=3,\n",
    "       metrics={'mlogloss'}, seed = 0,\n",
    "       callbacks=[xgb.callback.print_evaluation(show_stdv=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv-mlogloss: 0.988865+std0.00640186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
