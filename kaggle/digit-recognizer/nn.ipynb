{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic: Machine Learning from Disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "train, test = train_test_split(data, test_size=0.2,random_state=0, stratify=data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = train.loc[:, 'label'].as_matrix()\n",
    "Y_train = np.eye(10)[Y_train].T\n",
    "X_train = train.loc[:, train.columns != 'label'].as_matrix().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test = test.loc[:, 'label'].as_matrix()\n",
    "Y_test = np.eye(10)[Y_test].T\n",
    "X_test = test.loc[:, test.columns != 'label'].as_matrix().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.multiply(X_train, 1.0/255.0)\n",
    "X_test = np.multiply(X_test, 1.0/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_batch(initial, batch_size, X, Y):\n",
    "    return X[:, initial:initial+batch_size], Y[:, initial:initial+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Initializing parameters with `he initialization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    for l in range(1, len(layer_dims)):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2/layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    t = np.exp(Z)\n",
    "    return t / np.sum(t, axis=0)\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Forward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    linear_cache = (A_prev, W, b)\n",
    "\n",
    "    if activation == 'softmax':\n",
    "        A = softmax(Z)\n",
    "    elif activation == 'relu':\n",
    "        A = relu(Z)\n",
    "        \n",
    "    activation_cache = Z\n",
    "    cache = (linear_cache, activation_cache)\n",
    "#     print('LAF', A.shape)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def forward_propogation(X, parameters):\n",
    "    A_prev = X\n",
    "    L = len(parameters)//2\n",
    "    caches = []\n",
    "    for l in range(1, L):\n",
    "        Wl = parameters['W' + str(l)]\n",
    "        bl = parameters['b' + str(l)]\n",
    "        A_prev, cache = linear_activation_forward(A_prev, Wl, bl, 'relu')\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = linear_activation_forward(A_prev, parameters['W' + str(L)], parameters['b' + str(L)], 'softmax')\n",
    "    caches.append(cache)\n",
    "    return AL, caches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Cost Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#np.mulliply is diff than X*Y\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = -1 / m * (np.sum(Y*np.log(AL)))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost\n",
    "\n",
    "def compute_cost_with_regularization(AL, Y, parameters, lambd):\n",
    "    m = Y.shape[1]\n",
    "    cost = 0\n",
    "    for l in range(1, len(parameters)//2):\n",
    "        cost += np.sum(np.square(parameters['W' + str(l)]))\n",
    "\n",
    "    cross_entropy_cost = compute_cost(AL, Y)\n",
    "    L2_regularization_cost = lambd * cost / (2 * m)\n",
    "\n",
    "    return  cross_entropy_cost + L2_regularization_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Backward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA, activation_cache):\n",
    "    Z = activation_cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == 'softmax':\n",
    "        dZ = dA\n",
    "    elif activation == 'relu':\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        \n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1 / m * np.dot(dZ, A_prev.T)\n",
    "    db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def backward_propogation(AL, Y, caches):\n",
    "    L = len(caches)\n",
    "    grads = {}\n",
    "    dZ = AL - Y\n",
    "    grads['dA' + str(L)], grads['dW' + str(L)], grads['db' + str(L)] = linear_activation_backward(dZ, caches[L-1], 'softmax')\n",
    "    A_prev = AL\n",
    "    for l in range(L-1, 0, -1):\n",
    "        cache = caches[l-1]\n",
    "        dA = grads['dA' + str(l+1)]\n",
    "        dA_prev, dW, db = linear_activation_backward(dA, cache, 'relu')\n",
    "        grads['dA' + str(l)] = dA_prev\n",
    "        grads['dW' + str(l)] = dW\n",
    "        grads['db' + str(l)] = db\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def backward_propogation_with_regularization(AL, Y, caches, lambd):\n",
    "    L = len(caches)\n",
    "    grads = {}\n",
    "    m = Y.shape[1]\n",
    "    dZ = AL - Y\n",
    "    grads['dA' + str(L)], grads['dW' + str(L)], grads['db' + str(L)] = linear_activation_backward(dZ, caches[L-1], 'softmax')\n",
    "    grads['dW' + str(L)] += (lambd * caches[L-1][0][1]) / m\n",
    "    A_prev = AL\n",
    "\n",
    "    for l in range(L-1, 0, -1):\n",
    "        cache = caches[l-1]\n",
    "        dA = grads['dA' + str(l+1)]\n",
    "        dA_prev, dW, db = linear_activation_backward(dA, cache, 'relu')\n",
    "        grads['dA' + str(l)] = dA_prev\n",
    "        grads['dW' + str(l)] = dW + (lambd * cache[0][1]) / m\n",
    "        grads['db' + str(l)] = db\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Updating parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    for l in range(1, len(parameters)//2 + 1 ):\n",
    "        parameters['W' + str(l)] -= learning_rate * grads['dW' + str(l)]\n",
    "        parameters['b' + str(l)] -= learning_rate * grads['db' + str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def the_model(X_train, Y_train, layers_dims, learning_rate, num_iterations, lambd):\n",
    "    costs = []\n",
    "    initial = 0\n",
    "    batch_size = 1000\n",
    "    parameters = Initialize_parameters_deep(layers_dims)\n",
    "\n",
    "    for i in range(num_iterations+1):\n",
    "        X, Y = get_next_batch(initial, batch_size, X_train, Y_train)\n",
    "#         print(initial, X.shape, Y.shape)\n",
    "        initial = 0 if initial>=33000 else initial+batch_size\n",
    "#         print(initial)\n",
    "        AL, caches = forward_propogation(X, parameters)\n",
    "        if lambd == 0:\n",
    "            cost = compute_cost(AL, Y)\n",
    "            grads = backward_propogation(AL, Y, caches)\n",
    "        else:\n",
    "            cost = compute_cost_with_regularization(AL, Y, parameters, lambd)\n",
    "            grads = backward_propogation_with_regularization(AL, Y, caches, lambd)\n",
    "            \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        if (i%500==0):\n",
    "            print('Cost at iteration %s is %s' %(i, cost))\n",
    "\n",
    "        costs.append(cost)\n",
    "            \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per 10000)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    np.save(\"parameters\", parameters)\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "layers_dims = [784, 128, 64, 10]\n",
    "learning_rate = 0.5\n",
    "num_iterations = 2001\n",
    "lambd = 0\n",
    "parameters = the_model(X_train, Y_train, layers_dims, learning_rate, num_iterations, lambd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(X, Y, parameters):\n",
    "    Y_hat, _ = forward_propogation(X, parameters)\n",
    "    m = Y.shape[1]\n",
    "    acc = np.sum(np.argmax(Y, axis=0) == np.argmax(Y_hat, axis=0)) / m\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "accuracy(X_train, Y_train, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "accuracy(X_test, Y_test, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_data = pd.read_csv('test.csv')\n",
    "X_test_data = np.multiply(X_test_data, 1.0/255.0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, _ = forward_propogation(X_test_data, parameters)\n",
    "Y = np.argmax(Y, axis=0)\n",
    "with open('submission-nn-v1.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['ImageId', 'Label'])\n",
    "    for i in range(Y.shape[0]):\n",
    "        writer.writerow([i+1, Y[i]])\n",
    "\n",
    "print(\"Done Writing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ML]",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
