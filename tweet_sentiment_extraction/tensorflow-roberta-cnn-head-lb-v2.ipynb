{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow roBERTa + CNN head - LB   v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello everyone! \n",
    "\n",
    "1. 1. 1. This kernel is based on [Al-Kharba Kiram](https://www.kaggle.com/al0kharba/tensorflow-roberta-0-712/output).  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load  data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 1.14.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train():\n",
    "    train=pd.read_csv('data/train.csv')\n",
    "    train['text']=train['text'].astype(str)\n",
    "    train['selected_text']=train['selected_text'].astype(str)\n",
    "    return train\n",
    "\n",
    "def read_test():\n",
    "    test=pd.read_csv('data/test.csv')\n",
    "    test['text']=test['text'].astype(str)\n",
    "    return test\n",
    "\n",
    "def read_submission():\n",
    "    test=pd.read_csv('data/sample_submission.csv')\n",
    "    return test\n",
    "    \n",
    "train_df = read_train()\n",
    "test_df = read_test()\n",
    "submission_df = read_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_improve(str1, str2): \n",
    "    str1=str1.lower()\n",
    "    str2=str2.lower()    \n",
    "    index=str1.find(str2) \n",
    "    text1=str1[:index]\n",
    "    #print(text1)\n",
    "    text2=str1[index:].replace(str2,'')\n",
    "    words1=text1.split()\n",
    "    words2=text2.split()\n",
    "    #print(words1[-3:])\n",
    "\n",
    "    if len(words1)>len(words2):\n",
    "        words1=words1[-3:]\n",
    "        mod_text=\" \".join(words1)+\" \"+ str2\n",
    "    else:\n",
    "        words2=words2[0:2]\n",
    "        mod_text=str2+\" \"+\" \".join(words2)\n",
    "    return mod_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str(str1).lower().split())  \n",
    "    b = set(str(str2).lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27486\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df))\n",
    "#train_df1=train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['selected_text_mod']=train_df['selected_text']\n",
    "train_df['mod']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['mod']=0\n",
    "# for index,row in train_df.iterrows():\n",
    "#     #print(row['text'])\n",
    "#     #print(row['selected_text'])\n",
    "#     res1=jaccard(row['text'],row['selected_text_mod'])\n",
    "#     res2=jaccard(row['text'],row['selected_text'])\n",
    "    \n",
    "#     if res1<0.5 and row['mod']==0:\n",
    "#         mod_text=jaccard_improve(row['text'],row['selected_text'])\n",
    "#         train_df.at[index,'mod']=1\n",
    "#         train_df.at[index,'selected_text']=mod_text\n",
    "# #         print('____________1')\n",
    "# #         print(mod_text)\n",
    "# #         print(row['text'])\n",
    "# #         print(row['selected_text'])\n",
    "# #         print('____________2')\n",
    "#         res2=jaccard(row['text'],mod_text)\n",
    "#     else:\n",
    "#         train_df.at[index,'selected_text']=row['selected_text_mod']\n",
    "    \n",
    "#     train_df.at[index,'score1']=res1\n",
    "#     train_df.at[index,'score2']=res2\n",
    "    \n",
    "#     #print(res1)\n",
    "    \n",
    "#     #print(res1)\n",
    "#     #train_df.at[index,'score']=res1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_df[train_df.score1!=train_df.score2]))\n",
    "\n",
    "# train_df[train_df.score1!=train_df.score2]\n",
    "\n",
    "# #print(len(train_df[train_df.score>0.9]))\n",
    "# train_df2=train_df[train_df.score>0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = train_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preproccesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Error while initializing BPE: The system cannot find the file specified. (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-92bb778f237f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmerges_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'merges-roberta-base.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0madd_prefix_space\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[0msentiment_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'positive'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1313\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'negative'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m2430\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'neutral'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m7974\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tokenizers\\implementations\\byte_level_bpe.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab_file, merges_file, add_prefix_space, lowercase, dropout, unicode_normalizer, continuing_subword_prefix, end_of_word_suffix, trim_offsets)\u001b[0m\n\u001b[0;32m     32\u001b[0m                     \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                     \u001b[0mcontinuing_subword_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcontinuing_subword_prefix\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                     \u001b[0mend_of_word_suffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_of_word_suffix\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                 )\n\u001b[0;32m     36\u001b[0m             )\n",
      "\u001b[1;31mException\u001b[0m: Error while initializing BPE: The system cannot find the file specified. (os error 2)"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 96\n",
    "PATH = 'data/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = train_df.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(token_type_ids.shape)\n",
    "print(start_tokens.shape)\n",
    "print(end_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "# for k in range(train_df.shape[0]):\n",
    "    \n",
    "#     # FIND OVERLAP\n",
    "#     text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
    "#     text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n",
    "#     idx = text1.find(text2)\n",
    "#     #text1='rx th as'\n",
    "#     chars = np.zeros((len(text1)))\n",
    "#     print(\"========1\")\n",
    "#     print(chars)\n",
    "#     chars[idx:idx+len(text2)]=1\n",
    "#     print(chars)\n",
    "#     print(idx)\n",
    "#     print(text1)\n",
    "#     print(text2)\n",
    "#     print(len(text2))\n",
    "#     enc = tokenizer.encode(text1) \n",
    "#     print(enc)\n",
    "#     print(text1)\n",
    "#     if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "#     print(chars)\n",
    "\n",
    "#     offsets = []; idx=0    \n",
    "#     for t in enc.ids:\n",
    "#         w = tokenizer.decode([t])\n",
    "#         #print(w)\n",
    "#         #print(len(w))\n",
    "#         offsets.append((idx,idx+len(w)))\n",
    "#         idx += len(w)\n",
    "#     #print(offsets)\n",
    "        \n",
    "        \n",
    "#     #offsets.append((idx,idx+len(w)))\n",
    "#     #idx += len(w)\n",
    "        \n",
    "#     # START END TOKENS\n",
    "#     toks = []\n",
    "#     for i,(a,b) in enumerate(offsets):\n",
    "#         #print(a,b)\n",
    "#         sm = np.sum(chars[a:b])\n",
    "#         #print(chars[a:b])\n",
    "#         #print(sm)\n",
    "#         if sm>0: toks.append(i) \n",
    "\n",
    "#     s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n",
    "#     input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "#     attention_mask[k,:len(enc.ids)+5] = 1\n",
    "#     if len(toks)>0:\n",
    "#         start_tokens[k,toks[0]+1] = 1\n",
    "#         end_tokens[k,toks[-1]+1] = 1            \n",
    "            \n",
    "#     print(\"========21\")   \n",
    "#     print(enc.ids)\n",
    "#     print(enc)    \n",
    "#     print(text1)\n",
    "#     print(text2)\n",
    "    \n",
    "#     print(offsets)    \n",
    "#     print(chars)\n",
    "#     print(len(chars))\n",
    "#     print(toks)\n",
    "#     print(len(toks))\n",
    "\n",
    "#     print(input_ids[k,:] )\n",
    "#     print(s_tok)\n",
    "#     print( start_tokens[k,])\n",
    "#     print( end_tokens[k,])\n",
    "#     print(attention_mask[k,:])\n",
    "#     print(toks)\n",
    "#     print([0] + enc.ids + [2,2] + [s_tok] + [2])\n",
    "#     print(\"========2\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = train_df.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train_df.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        end_tokens[k,toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = test_df.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test_df.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test_df.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    return 3e-5 * 0.2**epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    #x1 = tf.keras.layers.ReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    #x2 = tf.keras.layers.ReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "We will skip this stage and load already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #input_ids\n",
    "# #train_df.sentiment.values\n",
    "\n",
    "# print(len(train_df))\n",
    "\n",
    "# train_df1=train_df[:1000]\n",
    "# print(len(train_df1))\n",
    "\n",
    "# input_ids1=input_ids[:1000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# skf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777)\n",
    "# for fold,(idxT,idxV) in enumerate(skf.split(input_ids1,train_df1.sentiment.values)):\n",
    "#     print(idxV)\n",
    "#     print(len(idxV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# jac = []; VER='v6'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "# oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "# oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777)\n",
    "\n",
    "# #for fold,(idxT,idxV) in enumerate(skf.split(input_ids1,train_df1.sentiment.values)):\n",
    "# for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):\n",
    "\n",
    "#     print('#'*25)\n",
    "#     print('### FOLD %i'%(fold+1))\n",
    "#     print('#'*25)\n",
    "    \n",
    "#     K.clear_session()\n",
    "#     model = build_model()\n",
    "        \n",
    "#     reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "#     sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "#         '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, \n",
    "#         save_best_only=True,\n",
    "#         save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "#     hist = model.fit([input_ids[idxT,], attention_mask[idxT,], \n",
    "#                       token_type_ids[idxT,]], [start_tokens[idxT,], \n",
    "#                                                end_tokens[idxT,]], \n",
    "#                         epochs=5, batch_size=8, verbose=DISPLAY, \n",
    "#                      callbacks=[sv, reduce_lr],\n",
    "#         validation_data=([input_ids[idxV,],attention_mask[idxV,],\n",
    "#                           token_type_ids[idxV,]], \n",
    "#         [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "#     print('Loading model...')\n",
    "#     model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "#     print('Predicting OOF...')\n",
    "#     oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "     \n",
    "    \n",
    "#     # DISPLAY FOLD JACCARD\n",
    "#     all = []\n",
    "#     for k in idxV:\n",
    "#         a = np.argmax(oof_start[k,])\n",
    "#         b = np.argmax(oof_end[k,])\n",
    "#         if a>b: \n",
    "#             st = train_df.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "#         else:\n",
    "#             text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
    "#             enc = tokenizer.encode(text1)\n",
    "#             st = tokenizer.decode(enc.ids[a-1:b])\n",
    "#         all.append(jaccard(st,train_df.loc[k,'selected_text']))\n",
    "#     jac.append(np.mean(all))\n",
    "#     print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "#     print(oof_start[idxV,])\n",
    "#     print(oof_end[idxV,])\n",
    "    \n",
    "     \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "# # DISPLAY FOLD JACCARD\n",
    "# all = []\n",
    "# for k in idxV:\n",
    "#     a = np.argmax(oof_start[k,])\n",
    "#     b = np.argmax(oof_end[k,])\n",
    "#     if a>b: \n",
    "#         st = train_df.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "#     else:\n",
    "#         text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
    "#         enc = tokenizer.encode(text1)\n",
    "#         st = tokenizer.decode(enc.ids[a-1:b])\n",
    "#     all.append(jaccard(st,train_df.loc[k,'selected_text']))\n",
    "# jac.append(np.mean(all))\n",
    "# print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "# # print(oof_start[idxV,])\n",
    "# # print(oof_end[idxV,])\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.loc[10,'text']\n",
    "\n",
    "\n",
    "#train_df.reset_index(inplace = True) \n",
    "#train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "DISPLAY=1\n",
    "for i in range(5):\n",
    "    print('#'*25)\n",
    "    print('### MODEL %i'%(i+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "    #model.load_weights('../input/m6aprila/v6-roberta-%i.h5'%i)\n",
    "    model.load_weights('../input/model8/v8-roberta-%i.h5'%i)\n",
    "\n",
    "    #model.load_weights('v5-roberta-%i.h5'%i)\n",
    "\n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/n_splits\n",
    "    preds_end += preds[1]/n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test_df.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])          \n",
    "        st1=st.strip()\n",
    "        if st1=='****' or  st1=='****!' or st1=='****!' or st1=='****!' or st1=='****,' or  st1=='****,' or  st1=='****.' or st1=='****.':\n",
    "            #print(st1.strip())\n",
    "            #print(text1)\n",
    "            st=text1\n",
    "        elif st1=='(good':   \n",
    "            st='good'\n",
    "        elif st1=='__joy':   \n",
    "            st='joy'           \n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # train_df.to_csv('train_df.csv',index=False)\n",
    "\n",
    "# test_df=pd.read_csv('../input/submission/submission_v2.csv')\n",
    "\n",
    "# # for index,row in test_df.iterrows():\n",
    "# #     row['selected_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# for index,row in test_df.iterrows():\n",
    "#     #print(row['selected_text'])\n",
    "#     if len(row['selected_text'])>100:\n",
    "#         #print(row['selected_text'])\n",
    "#         test_df.at[index,'selected_text']=''\n",
    "#         i=i+1\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['selected_text'] = all\n",
    "test_df[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from distutils.dir_util import copy_tree\n",
    "# todir='/kaggle/working'\n",
    "# fromdirc='../input/tweet-sentiment-extraction'\n",
    "# copy_tree(fromdirc,todir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('/kaggle')\n",
    "# os.getcwd()\n",
    "# os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import os\n",
    "# # os.getcwd()\n",
    "\n",
    "# import shutil\n",
    "# source='/kaggle/test1.csv'\n",
    "# destination='/kaggle/working/test1.csv'\n",
    "# dest = shutil.copyfile(source, destination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# for subdir, dirs, files in os.walk('/kaggle/working/'):\n",
    "#     for file in files:\n",
    "#         if '.h5' in file:\n",
    "#           #print(file) #file\n",
    "#           source='/kaggle/working'+str('/')+file\n",
    "#           destination='../input/model4'+str('/')+file\n",
    "#           destination='/kaggle'+str('/')+file\n",
    "#           print(source)  \n",
    "#           #print(destination)  \n",
    "#           dest = shutil.copyfile(source, destination) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# for subdir, dirs, files in os.walk('../input/model4/'):\n",
    "#     for file in files:\n",
    "#       print(file) #file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import FileLink, FileLinks\n",
    "# FileLinks('.') #lists all downloadable files on server"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
